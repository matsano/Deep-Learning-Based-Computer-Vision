{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Fashion MNIST dataset using a VQ VAE**\n",
        "\n",
        "- RUBACK, Arthur\n",
        "- SANO, Matheus"
      ],
      "metadata": {
        "id": "TjOUMDzJ7h80"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1) Connection to your drive**\n",
        "\n"
      ],
      "metadata": {
        "id": "Yi_AjIIFcDn2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXOTxzVuR2Wo",
        "outputId": "722245d4-d5a6-47de-f119-55c98b447f1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2) Clone the repository**\n",
        "\n"
      ],
      "metadata": {
        "id": "epX7yBudeN1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/nadavbh12/VQ-VAE.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3_GT-Ulct5i",
        "outputId": "d1652759-9be6-4cce-9fb4-525abf588091"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'VQ-VAE'...\n",
            "remote: Enumerating objects: 195, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 195 (delta 4), reused 10 (delta 3), pack-reused 180\u001b[K\n",
            "Receiving objects: 100% (195/195), 530.54 KiB | 5.15 MiB/s, done.\n",
            "Resolving deltas: 100% (109/109), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3) Install the requirements**"
      ],
      "metadata": {
        "id": "Buo9zl_Gecpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd VQ-VAE/\n",
        "! pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEdeyrwKddfS",
        "outputId": "d26c09c4-7357-4d4c-b294-0fb994a295cc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/VQ-VAE\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.16.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (4.66.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->-r requirements.txt (line 2)) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->-r requirements.txt (line 2)) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->-r requirements.txt (line 2)) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 1)) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r requirements.txt (line 2)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r requirements.txt (line 2)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r requirements.txt (line 2)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r requirements.txt (line 2)) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 1)) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4) Useful libraries**\n",
        "\n",
        "Import of standard Python libraries and libraries specific to deep learning."
      ],
      "metadata": {
        "id": "-tITvXt88IFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import logging\n",
        "import argparse\n",
        "\n",
        "import torch.utils.data\n",
        "from torch import optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "\n",
        "from vq_vae.util import setup_logging_from_args\n",
        "from vq_vae.auto_encoder import *"
      ],
      "metadata": {
        "id": "FvNg7zm58NnD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a7fb9e4-5af0-48d0-c856-b27e6764a7b0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/VQ-VAE/vq_vae/util.py:26: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if save_name is '':\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5) Models definition**\n",
        "\n",
        "Definition of different autoencoder models for specific data sets, such as VAE, VQ-VAE."
      ],
      "metadata": {
        "id": "2Z-onXBu9FlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    'custom': {'vqvae': VQ_CVAE,\n",
        "               'vqvae2': VQ_CVAE2},\n",
        "    'imagenet': {'vqvae': VQ_CVAE,\n",
        "                 'vqvae2': VQ_CVAE2},\n",
        "    'cifar10': {'vae': CVAE,\n",
        "                'vqvae': VQ_CVAE,\n",
        "                'vqvae2': VQ_CVAE2},\n",
        "    'mnist': {'vae': VAE,\n",
        "              'vqvae': VQ_CVAE},\n",
        "    'fashion_mnist' : {'vae': VAE,\n",
        "                       'vqvae': VQ_CVAE},\n",
        "}"
      ],
      "metadata": {
        "id": "zqFQ90i29hAN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6) Data Set Configurations**\n",
        "\n",
        "Definition of specific classes, transformations and hyperparameters for different data sets, such as Fashion MNIST."
      ],
      "metadata": {
        "id": "tnnlUkLp9Z_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datasets_classes = {\n",
        "    'custom': datasets.ImageFolder,\n",
        "    'imagenet': datasets.ImageFolder,\n",
        "    'cifar10': datasets.CIFAR10,\n",
        "    'mnist': datasets.MNIST,\n",
        "    'fashion_mnist': datasets.FashionMNIST,\n",
        "}\n",
        "dataset_train_args = {\n",
        "    'custom': {},\n",
        "    'imagenet': {},\n",
        "    'cifar10': {'train': True, 'download': True},\n",
        "    'mnist': {'train': True, 'download': True},\n",
        "    'fashion_mnist': {'train': True, 'download': True},\n",
        "}\n",
        "dataset_test_args = {\n",
        "    'custom': {},\n",
        "    'imagenet': {},\n",
        "    'cifar10': {'train': False, 'download': True},\n",
        "    'mnist': {'train': False, 'download': True},\n",
        "    'fashion_mnist': {'train': False, 'download': True}\n",
        "}\n",
        "dataset_n_channels = {\n",
        "    'custom': 3,\n",
        "    'imagenet': 3,\n",
        "    'cifar10': 3,\n",
        "    'mnist': 1,\n",
        "    'fashion_mnist': 1,\n",
        "}\n",
        "\n",
        "dataset_transforms = {\n",
        "    'custom': transforms.Compose([transforms.Resize(256), transforms.CenterCrop(256),\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),\n",
        "    'imagenet': transforms.Compose([transforms.Resize(256), transforms.CenterCrop(256),\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),\n",
        "    'cifar10': transforms.Compose([transforms.ToTensor(),\n",
        "                                   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),\n",
        "    'mnist': transforms.ToTensor(),\n",
        "    'fashion_mnist': transforms.Compose([transforms.ToTensor(),\n",
        "                                   transforms.Normalize((0.5,), (0.5,))]),\n",
        "}\n",
        "\n",
        "default_hyperparams = {\n",
        "    'custom': {'lr': 2e-4, 'k': 512, 'hidden': 128},\n",
        "    'imagenet': {'lr': 2e-4, 'k': 512, 'hidden': 128},\n",
        "    'cifar10': {'lr': 2e-4, 'k': 10, 'hidden': 256},\n",
        "    'mnist': {'lr': 1e-4, 'k': 10, 'hidden': 64},\n",
        "    'fashion_mnist': {'lr': 1e-4, 'k': 10, 'hidden': 64}\n",
        "}"
      ],
      "metadata": {
        "id": "rHqAM8Z9_hjy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7) Standard Hyperparameter Configuration**\n",
        "\n",
        "Definition of standard hyperparameters for different data sets"
      ],
      "metadata": {
        "id": "Hg5m1qNK_iWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args_init = argparse.ArgumentParser(description='parser pour vqvae')\n",
        "args_init.add_argument('--dataset', default='fashion_mnist')\n",
        "args_init.add_argument('--model', default='vqvae')\n",
        "args_init.add_argument('--data-dir', default='~/.datasets')\n",
        "args_init.add_argument('--epochs', default='10')\n",
        "\n",
        "command_line_args = [\"--dataset\", \"fashion_mnist\", \"--model\", \"vqvae\", \"--epochs\", \"5\"]\n",
        "\n",
        "parsed_args = args_init.parse_args(command_line_args)"
      ],
      "metadata": {
        "id": "UOa_7uQ5_z6L"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8) Model training**\n",
        "\n",
        "Model training for one epoch"
      ],
      "metadata": {
        "id": "W5J-6PmTFzwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch, model, train_loader, optimizer, cuda, log_interval, save_path, args, writer):\n",
        "    model.train()\n",
        "    loss_dict = model.latest_losses()\n",
        "    losses = {k + '_train': 0 for k, v in loss_dict.items()}\n",
        "    epoch_losses = {k + '_train': 0 for k, v in loss_dict.items()}\n",
        "    start_time = time.time()\n",
        "    batch_idx, data = None, None\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        if cuda:\n",
        "            data = data.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data)\n",
        "        loss = model.loss_function(data, *outputs)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        latest_losses = model.latest_losses()\n",
        "        for key in latest_losses:\n",
        "            losses[key + '_train'] += float(latest_losses[key])\n",
        "            epoch_losses[key + '_train'] += float(latest_losses[key])\n",
        "\n",
        "        if batch_idx % log_interval == 0:\n",
        "            for key in latest_losses:\n",
        "                losses[key + '_train'] /= log_interval\n",
        "            loss_string = ' '.join(['{}: {:.6f}'.format(k, v) for k, v in losses.items()])\n",
        "            logging.info('Train Epoch: {epoch} [{batch:5d}/{total_batch} ({percent:2d}%)]   time:'\n",
        "                         ' {time:3.2f}   {loss}'\n",
        "                         .format(epoch=epoch, batch=batch_idx * len(data), total_batch=len(train_loader) * len(data),\n",
        "                                 percent=int(100. * batch_idx / len(train_loader)),\n",
        "                                 time=time.time() - start_time,\n",
        "                                 loss=loss_string))\n",
        "            start_time = time.time()\n",
        "            # logging.info('z_e norm: {:.2f}'.format(float(torch.mean(torch.norm(outputs[1][0].contiguous().view(256,-1),2,0)))))\n",
        "            # logging.info('z_q norm: {:.2f}'.format(float(torch.mean(torch.norm(outputs[2][0].contiguous().view(256,-1),2,0)))))\n",
        "            for key in latest_losses:\n",
        "                losses[key + '_train'] = 0\n",
        "        if batch_idx == (len(train_loader) - 1):\n",
        "            save_reconstructed_images(data, epoch, outputs[0], save_path, 'reconstruction_train')\n",
        "\n",
        "            write_images(data, outputs, writer, 'train')\n",
        "\n",
        "        if args.dataset in ['imagenet', 'custom'] and batch_idx * len(data) > args.max_epoch_samples:\n",
        "            break\n",
        "\n",
        "    for key in epoch_losses:\n",
        "        if args.dataset != 'imagenet':\n",
        "            epoch_losses[key] /= (len(train_loader.dataset) / train_loader.batch_size)\n",
        "        else:\n",
        "            epoch_losses[key] /= (len(train_loader.dataset) / train_loader.batch_size)\n",
        "    loss_string = '\\t'.join(['{}: {:.6f}'.format(k, v) for k, v in epoch_losses.items()])\n",
        "    logging.info('====> Epoch: {} {}'.format(epoch, loss_string))\n",
        "    # if len(outputs) > 3:\n",
        "    #   writer.add_histogram('dict frequency', outputs[3], bins=range(args.k + 1))\n",
        "    #   model.print_atom_hist(outputs[3])\n",
        "    return epoch_losses\n"
      ],
      "metadata": {
        "id": "woDqxaNhGC20"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **9) Model evaluation**\n",
        "\n",
        "Evaluate the model on the test set after training for one epoch"
      ],
      "metadata": {
        "id": "Z8Y-c3TsGIh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_net(epoch, model, test_loader, cuda, save_path, args, writer):\n",
        "    model.eval()\n",
        "    loss_dict = model.latest_losses()\n",
        "    losses = {k + '_test': 0 for k, v in loss_dict.items()}\n",
        "    i, data = None, None\n",
        "    with torch.no_grad():\n",
        "        for i, (data, _) in enumerate(test_loader):\n",
        "            if cuda:\n",
        "                data = data.cuda()\n",
        "            outputs = model(data)\n",
        "            model.loss_function(data, *outputs)\n",
        "            latest_losses = model.latest_losses()\n",
        "            for key in latest_losses:\n",
        "                losses[key + '_test'] += float(latest_losses[key])\n",
        "            if i == 0:\n",
        "                write_images(data, outputs, writer, 'test')\n",
        "\n",
        "                save_reconstructed_images(data, epoch, outputs[0], save_path, 'reconstruction_test')\n",
        "                save_checkpoint(model, epoch, save_path)\n",
        "            if args.dataset == 'imagenet' and i * len(data) > 1000:\n",
        "                break\n",
        "\n",
        "    for key in losses:\n",
        "        if args.dataset not in ['imagenet', 'custom']:\n",
        "            losses[key] /= (len(test_loader.dataset) / test_loader.batch_size)\n",
        "        else:\n",
        "            losses[key] /= (i * len(data))\n",
        "    loss_string = ' '.join(['{}: {:.6f}'.format(k, v) for k, v in losses.items()])\n",
        "    logging.info('====> Test set losses: {}'.format(loss_string))\n",
        "    return losses"
      ],
      "metadata": {
        "id": "Orn4l8eyHtcH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **10) Auxiliary functions**\n",
        "\n",
        "Auxiliary functions used to save images and model checkpoints during model training and evaluation."
      ],
      "metadata": {
        "id": "oRMWL_KkIiyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def write_images(data, outputs, writer, suffix):\n",
        "    original = data.mul(0.5).add(0.5)\n",
        "    original_grid = make_grid(original[:6])\n",
        "    writer.add_image(f'original/{suffix}', original_grid)\n",
        "    reconstructed = outputs[0].mul(0.5).add(0.5)\n",
        "    reconstructed_grid = make_grid(reconstructed[:6])\n",
        "    writer.add_image(f'reconstructed/{suffix}', reconstructed_grid)\n",
        "\n",
        "\n",
        "def save_reconstructed_images(data, epoch, outputs, save_path, name):\n",
        "    size = data.size()\n",
        "    n = min(data.size(0), 8)\n",
        "    batch_size = data.size(0)\n",
        "    comparison = torch.cat([data[:n],\n",
        "                            outputs.view(batch_size, size[1], size[2], size[3])[:n]])\n",
        "    save_image(comparison.cpu(),\n",
        "               os.path.join(save_path, name + '_' + str(epoch) + '.png'), nrow=n, normalize=True)\n",
        "\n",
        "\n",
        "def save_checkpoint(model, epoch, save_path):\n",
        "    os.makedirs(os.path.join(save_path, 'checkpoints'), exist_ok=True)\n",
        "    checkpoint_path = os.path.join(save_path, 'checkpoints', f'model_{epoch}.pth')\n",
        "    torch.save(model.state_dict(), checkpoint_path)"
      ],
      "metadata": {
        "id": "Y7kNbh2UI0TK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **11) Main program**"
      ],
      "metadata": {
        "id": "EPSnGyKR_2gH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(args):\n",
        "    parser = argparse.ArgumentParser(description='Variational AutoEncoders')\n",
        "\n",
        "    model_parser = parser.add_argument_group('Model Parameters')\n",
        "    model_parser.add_argument('--model', default='vqvae', choices=['vae', 'vqvae'],\n",
        "                              help='autoencoder variant to use: vae | vqvae') # model_parser.model -> vae\n",
        "    model_parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
        "                              help='input batch size for training (default: 128)')\n",
        "    model_parser.add_argument('--hidden', type=int, metavar='N',\n",
        "                              help='number of hidden channels')\n",
        "    model_parser.add_argument('-k', '--dict-size', type=int, dest='k', metavar='K',\n",
        "                              help='number of atoms in dictionary')\n",
        "    model_parser.add_argument('--lr', type=float, default=None,\n",
        "                              help='learning rate')\n",
        "    model_parser.add_argument('--vq_coef', type=float, default=None,\n",
        "                              help='vq coefficient in loss')\n",
        "    model_parser.add_argument('--commit_coef', type=float, default=None,\n",
        "                              help='commitment coefficient in loss')\n",
        "    model_parser.add_argument('--kl_coef', type=float, default=None,\n",
        "                              help='kl-divergence coefficient in loss')\n",
        "\n",
        "    training_parser = parser.add_argument_group('Training Parameters')\n",
        "    training_parser.add_argument('--dataset', default='fashion_mnist', choices=['fashion_mnist', 'mnist', 'cifar10', 'imagenet',\n",
        "                                                                          'custom'],\n",
        "                                 help='dataset to use: fashion_mnist | mnist | cifar10 | imagenet | custom')\n",
        "    training_parser.add_argument('--dataset_dir_name', default='',\n",
        "                                 help='name of the dir containing the dataset if dataset == custom')\n",
        "    training_parser.add_argument('--data-dir', default='/media/ssd/Datasets',\n",
        "                                 help='directory containing the dataset')\n",
        "    training_parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
        "                                 help='number of epochs to train (default: 10)')\n",
        "    training_parser.add_argument('--max-epoch-samples', type=int, default=50000,\n",
        "                                 help='max num of samples per epoch')\n",
        "    training_parser.add_argument('--no-cuda', action='store_true', default=False,\n",
        "                                 help='enables CUDA training')\n",
        "    training_parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
        "                                 help='random seed (default: 1)')\n",
        "    training_parser.add_argument('--gpus', default='0',\n",
        "                                 help='gpus used for training - e.g 0,1,3')\n",
        "\n",
        "    logging_parser = parser.add_argument_group('Logging Parameters')\n",
        "    logging_parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
        "                                help='how many batches to wait before logging training status')\n",
        "    logging_parser.add_argument('--results-dir', metavar='RESULTS_DIR', default='./results',\n",
        "                                help='results dir')\n",
        "    logging_parser.add_argument('--save-name', default='',\n",
        "                                help='saved folder')\n",
        "    logging_parser.add_argument('--data-format', default='json',\n",
        "                                help='in which format to save the data')\n",
        "\n",
        "    args, _ = parser.parse_known_args()\n",
        "\n",
        "    args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "    dataset_dir_name = args.dataset if args.dataset != 'custom' else args.dataset_dir_name\n",
        "\n",
        "    lr = args.lr or default_hyperparams[args.dataset]['lr']\n",
        "    k = args.k or default_hyperparams[args.dataset]['k']\n",
        "    hidden = args.hidden or default_hyperparams[args.dataset]['hidden']\n",
        "    num_channels = dataset_n_channels[args.dataset]\n",
        "\n",
        "    save_path = setup_logging_from_args(args)\n",
        "    writer = SummaryWriter(save_path)\n",
        "\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.cuda:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "        args.gpus = [int(i) for i in args.gpus.split(',')]\n",
        "        torch.cuda.set_device(args.gpus[0])\n",
        "        cudnn.benchmark = True\n",
        "        torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "    model = models[args.dataset][args.model](hidden, k=k, num_channels=num_channels)\n",
        "    if args.cuda:\n",
        "        model.cuda()\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, 10 if args.dataset == 'imagenet' else 30, 0.5,)\n",
        "\n",
        "    kwargs = {'num_workers': 8, 'pin_memory': True} if args.cuda else {}\n",
        "    dataset_train_dir = os.path.join(args.data_dir, dataset_dir_name)\n",
        "    dataset_test_dir = os.path.join(args.data_dir, dataset_dir_name)\n",
        "    if args.dataset in ['imagenet', 'custom']:\n",
        "        dataset_train_dir = os.path.join(dataset_train_dir, 'train')\n",
        "        dataset_test_dir = os.path.join(dataset_test_dir, 'val')\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        datasets_classes[args.dataset](dataset_train_dir,\n",
        "                                       transform=dataset_transforms[args.dataset],\n",
        "                                       **dataset_train_args[args.dataset]),\n",
        "        batch_size=args.batch_size, shuffle=True, **kwargs)\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        datasets_classes[args.dataset](dataset_test_dir,\n",
        "                                       transform=dataset_transforms[args.dataset],\n",
        "                                       **dataset_test_args[args.dataset]),\n",
        "        batch_size=args.batch_size, shuffle=False, **kwargs)\n",
        "\n",
        "    print(\"Dir name =\", dataset_dir_name)\n",
        "\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        train_losses = train(epoch, model, train_loader, optimizer, args.cuda,\n",
        "                             args.log_interval, save_path, args, writer)\n",
        "        test_losses = test_net(epoch, model, test_loader, args.cuda, save_path, args, writer)\n",
        "\n",
        "        for k in train_losses.keys():\n",
        "            name = k.replace('_train', '')\n",
        "            train_name = k\n",
        "            test_name = k.replace('train', 'test')\n",
        "            writer.add_scalars(name, {'train': train_losses[train_name],\n",
        "                                      'test': test_losses[test_name],\n",
        "                                      })\n",
        "        scheduler.step()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "ogEm_otLNLup"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **12) Main execution**\n",
        "\n",
        "Generate the Fashion MNIST dataset using a VQ VAE"
      ],
      "metadata": {
        "id": "ZL3rNmvyJ7Ue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    modelTrained = main(parsed_args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdrHKz27KJ6f",
        "outputId": "ebfefe82-595b-4fd6-c429-3a2c5529c64e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /media/ssd/Datasets/fashion_mnist/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 15701238.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /media/ssd/Datasets/fashion_mnist/FashionMNIST/raw/train-images-idx3-ubyte.gz to /media/ssd/Datasets/fashion_mnist/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /media/ssd/Datasets/fashion_mnist/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 264978.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /media/ssd/Datasets/fashion_mnist/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /media/ssd/Datasets/fashion_mnist/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /media/ssd/Datasets/fashion_mnist/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 5134419.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /media/ssd/Datasets/fashion_mnist/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /media/ssd/Datasets/fashion_mnist/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /media/ssd/Datasets/fashion_mnist/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 11949240.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /media/ssd/Datasets/fashion_mnist/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /media/ssd/Datasets/fashion_mnist/FashionMNIST/raw\n",
            "\n",
            "Dir name = fashion_mnist\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Train Epoch: 1 [    0/60032 ( 0%)]   time: 5.20   mse_train: 0.169482 vq_train: 0.001238 commitment_train: 0.001238\n",
            "Train Epoch: 1 [  640/60032 ( 1%)]   time: 0.30   mse_train: 1.433081 vq_train: 0.012106 commitment_train: 0.012106\n",
            "Train Epoch: 1 [ 1280/60032 ( 2%)]   time: 0.22   mse_train: 0.999211 vq_train: 0.011494 commitment_train: 0.011494\n",
            "Train Epoch: 1 [ 1920/60032 ( 3%)]   time: 0.35   mse_train: 0.734349 vq_train: 0.010976 commitment_train: 0.010976\n",
            "Train Epoch: 1 [ 2560/60032 ( 4%)]   time: 0.28   mse_train: 0.560983 vq_train: 0.010732 commitment_train: 0.010732\n",
            "Train Epoch: 1 [ 3200/60032 ( 5%)]   time: 0.30   mse_train: 0.425639 vq_train: 0.010698 commitment_train: 0.010698\n",
            "Train Epoch: 1 [ 3840/60032 ( 6%)]   time: 0.31   mse_train: 0.327278 vq_train: 0.010854 commitment_train: 0.010854\n",
            "Train Epoch: 1 [ 4480/60032 ( 7%)]   time: 0.38   mse_train: 0.290317 vq_train: 0.011203 commitment_train: 0.011203\n",
            "Train Epoch: 1 [ 5120/60032 ( 8%)]   time: 0.27   mse_train: 0.263955 vq_train: 0.011528 commitment_train: 0.011528\n",
            "Train Epoch: 1 [ 5760/60032 ( 9%)]   time: 0.26   mse_train: 0.245323 vq_train: 0.011765 commitment_train: 0.011765\n",
            "Train Epoch: 1 [ 6400/60032 (10%)]   time: 0.28   mse_train: 0.235926 vq_train: 0.012016 commitment_train: 0.012016\n",
            "Train Epoch: 1 [ 7040/60032 (11%)]   time: 0.28   mse_train: 0.217248 vq_train: 0.012227 commitment_train: 0.012227\n",
            "Train Epoch: 1 [ 7680/60032 (12%)]   time: 0.28   mse_train: 0.214038 vq_train: 0.012594 commitment_train: 0.012594\n",
            "Train Epoch: 1 [ 8320/60032 (13%)]   time: 0.27   mse_train: 0.199701 vq_train: 0.012950 commitment_train: 0.012950\n",
            "Train Epoch: 1 [ 8960/60032 (14%)]   time: 0.32   mse_train: 0.194662 vq_train: 0.013459 commitment_train: 0.013459\n",
            "Train Epoch: 1 [ 9600/60032 (15%)]   time: 0.28   mse_train: 0.180866 vq_train: 0.013937 commitment_train: 0.013937\n",
            "Train Epoch: 1 [10240/60032 (17%)]   time: 0.27   mse_train: 0.182531 vq_train: 0.014637 commitment_train: 0.014637\n",
            "Train Epoch: 1 [10880/60032 (18%)]   time: 0.28   mse_train: 0.170414 vq_train: 0.015299 commitment_train: 0.015299\n",
            "Train Epoch: 1 [11520/60032 (19%)]   time: 0.29   mse_train: 0.166194 vq_train: 0.015712 commitment_train: 0.015712\n",
            "Train Epoch: 1 [12160/60032 (20%)]   time: 0.28   mse_train: 0.157822 vq_train: 0.016330 commitment_train: 0.016330\n",
            "Train Epoch: 1 [12800/60032 (21%)]   time: 0.30   mse_train: 0.156120 vq_train: 0.017260 commitment_train: 0.017260\n",
            "Train Epoch: 1 [13440/60032 (22%)]   time: 0.28   mse_train: 0.156391 vq_train: 0.018001 commitment_train: 0.018001\n",
            "Train Epoch: 1 [14080/60032 (23%)]   time: 0.28   mse_train: 0.141568 vq_train: 0.018490 commitment_train: 0.018490\n",
            "Train Epoch: 1 [14720/60032 (24%)]   time: 0.26   mse_train: 0.142858 vq_train: 0.019519 commitment_train: 0.019519\n",
            "Train Epoch: 1 [15360/60032 (25%)]   time: 0.31   mse_train: 0.132132 vq_train: 0.020231 commitment_train: 0.020231\n",
            "Train Epoch: 1 [16000/60032 (26%)]   time: 0.29   mse_train: 0.127996 vq_train: 0.020895 commitment_train: 0.020895\n",
            "Train Epoch: 1 [16640/60032 (27%)]   time: 0.28   mse_train: 0.126180 vq_train: 0.021580 commitment_train: 0.021580\n",
            "Train Epoch: 1 [17280/60032 (28%)]   time: 0.26   mse_train: 0.118972 vq_train: 0.022211 commitment_train: 0.022211\n",
            "Train Epoch: 1 [17920/60032 (29%)]   time: 0.29   mse_train: 0.117273 vq_train: 0.023058 commitment_train: 0.023058\n",
            "Train Epoch: 1 [18560/60032 (30%)]   time: 0.24   mse_train: 0.114722 vq_train: 0.023763 commitment_train: 0.023763\n",
            "Train Epoch: 1 [19200/60032 (31%)]   time: 0.20   mse_train: 0.115622 vq_train: 0.024464 commitment_train: 0.024464\n",
            "Train Epoch: 1 [19840/60032 (33%)]   time: 0.18   mse_train: 0.110190 vq_train: 0.025133 commitment_train: 0.025133\n",
            "Train Epoch: 1 [20480/60032 (34%)]   time: 0.21   mse_train: 0.108228 vq_train: 0.025790 commitment_train: 0.025790\n",
            "Train Epoch: 1 [21120/60032 (35%)]   time: 0.20   mse_train: 0.109780 vq_train: 0.026325 commitment_train: 0.026325\n",
            "Train Epoch: 1 [21760/60032 (36%)]   time: 0.18   mse_train: 0.105033 vq_train: 0.027229 commitment_train: 0.027229\n",
            "Train Epoch: 1 [22400/60032 (37%)]   time: 0.18   mse_train: 0.103043 vq_train: 0.027858 commitment_train: 0.027858\n",
            "Train Epoch: 1 [23040/60032 (38%)]   time: 0.19   mse_train: 0.106638 vq_train: 0.028494 commitment_train: 0.028494\n",
            "Train Epoch: 1 [23680/60032 (39%)]   time: 0.19   mse_train: 0.098047 vq_train: 0.029186 commitment_train: 0.029186\n",
            "Train Epoch: 1 [24320/60032 (40%)]   time: 0.17   mse_train: 0.100317 vq_train: 0.029880 commitment_train: 0.029880\n",
            "Train Epoch: 1 [24960/60032 (41%)]   time: 0.23   mse_train: 0.099743 vq_train: 0.030613 commitment_train: 0.030613\n",
            "Train Epoch: 1 [25600/60032 (42%)]   time: 0.20   mse_train: 0.097233 vq_train: 0.031391 commitment_train: 0.031391\n",
            "Train Epoch: 1 [26240/60032 (43%)]   time: 0.18   mse_train: 0.097960 vq_train: 0.032124 commitment_train: 0.032124\n",
            "Train Epoch: 1 [26880/60032 (44%)]   time: 0.20   mse_train: 0.098551 vq_train: 0.032708 commitment_train: 0.032708\n",
            "Train Epoch: 1 [27520/60032 (45%)]   time: 0.19   mse_train: 0.093039 vq_train: 0.033270 commitment_train: 0.033270\n",
            "Train Epoch: 1 [28160/60032 (46%)]   time: 0.22   mse_train: 0.092924 vq_train: 0.034272 commitment_train: 0.034272\n",
            "Train Epoch: 1 [28800/60032 (47%)]   time: 0.19   mse_train: 0.093838 vq_train: 0.034958 commitment_train: 0.034958\n",
            "Train Epoch: 1 [29440/60032 (49%)]   time: 0.19   mse_train: 0.092609 vq_train: 0.035555 commitment_train: 0.035555\n",
            "Train Epoch: 1 [30080/60032 (50%)]   time: 0.20   mse_train: 0.091209 vq_train: 0.036435 commitment_train: 0.036435\n",
            "Train Epoch: 1 [30720/60032 (51%)]   time: 0.19   mse_train: 0.090178 vq_train: 0.037178 commitment_train: 0.037178\n",
            "Train Epoch: 1 [31360/60032 (52%)]   time: 0.21   mse_train: 0.089004 vq_train: 0.037790 commitment_train: 0.037790\n",
            "Train Epoch: 1 [32000/60032 (53%)]   time: 0.20   mse_train: 0.091538 vq_train: 0.038840 commitment_train: 0.038840\n",
            "Train Epoch: 1 [32640/60032 (54%)]   time: 0.20   mse_train: 0.089821 vq_train: 0.039135 commitment_train: 0.039135\n",
            "Train Epoch: 1 [33280/60032 (55%)]   time: 0.19   mse_train: 0.087102 vq_train: 0.040067 commitment_train: 0.040067\n",
            "Train Epoch: 1 [33920/60032 (56%)]   time: 0.20   mse_train: 0.087730 vq_train: 0.040792 commitment_train: 0.040792\n",
            "Train Epoch: 1 [34560/60032 (57%)]   time: 0.21   mse_train: 0.087530 vq_train: 0.041366 commitment_train: 0.041366\n",
            "Train Epoch: 1 [35200/60032 (58%)]   time: 0.18   mse_train: 0.084593 vq_train: 0.042480 commitment_train: 0.042480\n",
            "Train Epoch: 1 [35840/60032 (59%)]   time: 0.20   mse_train: 0.085019 vq_train: 0.043186 commitment_train: 0.043186\n",
            "Train Epoch: 1 [36480/60032 (60%)]   time: 0.18   mse_train: 0.084628 vq_train: 0.044069 commitment_train: 0.044069\n",
            "Train Epoch: 1 [37120/60032 (61%)]   time: 0.19   mse_train: 0.083093 vq_train: 0.044367 commitment_train: 0.044367\n",
            "Train Epoch: 1 [37760/60032 (62%)]   time: 0.19   mse_train: 0.083300 vq_train: 0.045403 commitment_train: 0.045403\n",
            "Train Epoch: 1 [38400/60032 (63%)]   time: 0.20   mse_train: 0.085683 vq_train: 0.046334 commitment_train: 0.046334\n",
            "Train Epoch: 1 [39040/60032 (65%)]   time: 0.19   mse_train: 0.081553 vq_train: 0.046566 commitment_train: 0.046566\n",
            "Train Epoch: 1 [39680/60032 (66%)]   time: 0.20   mse_train: 0.084293 vq_train: 0.047733 commitment_train: 0.047733\n",
            "Train Epoch: 1 [40320/60032 (67%)]   time: 0.18   mse_train: 0.083364 vq_train: 0.048516 commitment_train: 0.048516\n",
            "Train Epoch: 1 [40960/60032 (68%)]   time: 0.20   mse_train: 0.081234 vq_train: 0.049306 commitment_train: 0.049306\n",
            "Train Epoch: 1 [41600/60032 (69%)]   time: 0.22   mse_train: 0.082642 vq_train: 0.050051 commitment_train: 0.050051\n",
            "Train Epoch: 1 [42240/60032 (70%)]   time: 0.20   mse_train: 0.081415 vq_train: 0.050769 commitment_train: 0.050769\n",
            "Train Epoch: 1 [42880/60032 (71%)]   time: 0.19   mse_train: 0.079722 vq_train: 0.051686 commitment_train: 0.051686\n",
            "Train Epoch: 1 [43520/60032 (72%)]   time: 0.16   mse_train: 0.080333 vq_train: 0.051970 commitment_train: 0.051970\n",
            "Train Epoch: 1 [44160/60032 (73%)]   time: 0.23   mse_train: 0.079894 vq_train: 0.053350 commitment_train: 0.053350\n",
            "Train Epoch: 1 [44800/60032 (74%)]   time: 0.21   mse_train: 0.082227 vq_train: 0.053765 commitment_train: 0.053765\n",
            "Train Epoch: 1 [45440/60032 (75%)]   time: 0.18   mse_train: 0.077676 vq_train: 0.054635 commitment_train: 0.054635\n",
            "Train Epoch: 1 [46080/60032 (76%)]   time: 0.20   mse_train: 0.078393 vq_train: 0.055419 commitment_train: 0.055419\n",
            "Train Epoch: 1 [46720/60032 (77%)]   time: 0.18   mse_train: 0.079408 vq_train: 0.056377 commitment_train: 0.056377\n",
            "Train Epoch: 1 [47360/60032 (78%)]   time: 0.19   mse_train: 0.079151 vq_train: 0.057229 commitment_train: 0.057229\n",
            "Train Epoch: 1 [48000/60032 (79%)]   time: 0.21   mse_train: 0.077939 vq_train: 0.057858 commitment_train: 0.057858\n",
            "Train Epoch: 1 [48640/60032 (81%)]   time: 0.19   mse_train: 0.076353 vq_train: 0.058843 commitment_train: 0.058843\n",
            "Train Epoch: 1 [49280/60032 (82%)]   time: 0.18   mse_train: 0.081104 vq_train: 0.059738 commitment_train: 0.059738\n",
            "Train Epoch: 1 [49920/60032 (83%)]   time: 0.18   mse_train: 0.078546 vq_train: 0.060351 commitment_train: 0.060351\n",
            "Train Epoch: 1 [50560/60032 (84%)]   time: 0.18   mse_train: 0.077181 vq_train: 0.061339 commitment_train: 0.061339\n",
            "Train Epoch: 1 [51200/60032 (85%)]   time: 0.28   mse_train: 0.076610 vq_train: 0.061825 commitment_train: 0.061825\n",
            "Train Epoch: 1 [51840/60032 (86%)]   time: 0.28   mse_train: 0.075094 vq_train: 0.062705 commitment_train: 0.062705\n",
            "Train Epoch: 1 [52480/60032 (87%)]   time: 0.27   mse_train: 0.077422 vq_train: 0.064047 commitment_train: 0.064047\n",
            "Train Epoch: 1 [53120/60032 (88%)]   time: 0.27   mse_train: 0.075934 vq_train: 0.064209 commitment_train: 0.064209\n",
            "Train Epoch: 1 [53760/60032 (89%)]   time: 0.27   mse_train: 0.074967 vq_train: 0.065420 commitment_train: 0.065420\n",
            "Train Epoch: 1 [54400/60032 (90%)]   time: 0.27   mse_train: 0.073438 vq_train: 0.066196 commitment_train: 0.066196\n",
            "Train Epoch: 1 [55040/60032 (91%)]   time: 0.28   mse_train: 0.075340 vq_train: 0.067414 commitment_train: 0.067414\n",
            "Train Epoch: 1 [55680/60032 (92%)]   time: 0.29   mse_train: 0.076073 vq_train: 0.067860 commitment_train: 0.067860\n",
            "Train Epoch: 1 [56320/60032 (93%)]   time: 0.29   mse_train: 0.072884 vq_train: 0.068904 commitment_train: 0.068904\n",
            "Train Epoch: 1 [56960/60032 (94%)]   time: 0.27   mse_train: 0.076041 vq_train: 0.069942 commitment_train: 0.069942\n",
            "Train Epoch: 1 [57600/60032 (95%)]   time: 0.29   mse_train: 0.073923 vq_train: 0.070366 commitment_train: 0.070366\n",
            "Train Epoch: 1 [58240/60032 (97%)]   time: 0.30   mse_train: 0.074375 vq_train: 0.071445 commitment_train: 0.071445\n",
            "Train Epoch: 1 [58880/60032 (98%)]   time: 0.26   mse_train: 0.074145 vq_train: 0.072455 commitment_train: 0.072455\n",
            "Train Epoch: 1 [59520/60032 (99%)]   time: 0.12   mse_train: 0.073744 vq_train: 0.073302 commitment_train: 0.073302\n",
            "====> Epoch: 1 mse_train: 0.152209\tvq_train: 0.037942\tcommitment_train: 0.037942\n",
            "====> Test set losses: mse_test: 0.072499 vq_test: 0.074213 commitment_test: 0.074213\n",
            "Train Epoch: 2 [    0/60032 ( 0%)]   time: 0.34   mse_train: 0.007043 vq_train: 0.007428 commitment_train: 0.007428\n",
            "Train Epoch: 2 [  640/60032 ( 1%)]   time: 0.27   mse_train: 0.071845 vq_train: 0.074717 commitment_train: 0.074717\n",
            "Train Epoch: 2 [ 1280/60032 ( 2%)]   time: 0.21   mse_train: 0.070871 vq_train: 0.075885 commitment_train: 0.075885\n",
            "Train Epoch: 2 [ 1920/60032 ( 3%)]   time: 0.18   mse_train: 0.073352 vq_train: 0.076735 commitment_train: 0.076735\n",
            "Train Epoch: 2 [ 2560/60032 ( 4%)]   time: 0.23   mse_train: 0.074782 vq_train: 0.077857 commitment_train: 0.077857\n",
            "Train Epoch: 2 [ 3200/60032 ( 5%)]   time: 0.20   mse_train: 0.071887 vq_train: 0.078412 commitment_train: 0.078412\n",
            "Train Epoch: 2 [ 3840/60032 ( 6%)]   time: 0.21   mse_train: 0.073899 vq_train: 0.079904 commitment_train: 0.079904\n",
            "Train Epoch: 2 [ 4480/60032 ( 7%)]   time: 0.22   mse_train: 0.067941 vq_train: 0.080159 commitment_train: 0.080159\n",
            "Train Epoch: 2 [ 5120/60032 ( 8%)]   time: 0.20   mse_train: 0.068788 vq_train: 0.081473 commitment_train: 0.081473\n",
            "Train Epoch: 2 [ 5760/60032 ( 9%)]   time: 0.19   mse_train: 0.071468 vq_train: 0.082465 commitment_train: 0.082465\n",
            "Train Epoch: 2 [ 6400/60032 (10%)]   time: 0.22   mse_train: 0.069369 vq_train: 0.083166 commitment_train: 0.083166\n",
            "Train Epoch: 2 [ 7040/60032 (11%)]   time: 0.21   mse_train: 0.069637 vq_train: 0.083981 commitment_train: 0.083981\n",
            "Train Epoch: 2 [ 7680/60032 (12%)]   time: 0.20   mse_train: 0.068984 vq_train: 0.084809 commitment_train: 0.084809\n",
            "Train Epoch: 2 [ 8320/60032 (13%)]   time: 0.17   mse_train: 0.068698 vq_train: 0.086011 commitment_train: 0.086011\n",
            "Train Epoch: 2 [ 8960/60032 (14%)]   time: 0.22   mse_train: 0.069585 vq_train: 0.087105 commitment_train: 0.087105\n",
            "Train Epoch: 2 [ 9600/60032 (15%)]   time: 0.20   mse_train: 0.070807 vq_train: 0.088505 commitment_train: 0.088505\n",
            "Train Epoch: 2 [10240/60032 (17%)]   time: 0.19   mse_train: 0.071578 vq_train: 0.088603 commitment_train: 0.088603\n",
            "Train Epoch: 2 [10880/60032 (18%)]   time: 0.21   mse_train: 0.068785 vq_train: 0.089567 commitment_train: 0.089567\n",
            "Train Epoch: 2 [11520/60032 (19%)]   time: 0.19   mse_train: 0.067487 vq_train: 0.090807 commitment_train: 0.090807\n",
            "Train Epoch: 2 [12160/60032 (20%)]   time: 0.19   mse_train: 0.070660 vq_train: 0.092069 commitment_train: 0.092069\n",
            "Train Epoch: 2 [12800/60032 (21%)]   time: 0.21   mse_train: 0.068417 vq_train: 0.092735 commitment_train: 0.092735\n",
            "Train Epoch: 2 [13440/60032 (22%)]   time: 0.18   mse_train: 0.067259 vq_train: 0.093851 commitment_train: 0.093851\n",
            "Train Epoch: 2 [14080/60032 (23%)]   time: 0.23   mse_train: 0.068621 vq_train: 0.094648 commitment_train: 0.094648\n",
            "Train Epoch: 2 [14720/60032 (24%)]   time: 0.19   mse_train: 0.063608 vq_train: 0.095191 commitment_train: 0.095191\n",
            "Train Epoch: 2 [15360/60032 (25%)]   time: 0.18   mse_train: 0.067940 vq_train: 0.096925 commitment_train: 0.096925\n",
            "Train Epoch: 2 [16000/60032 (26%)]   time: 0.21   mse_train: 0.065670 vq_train: 0.097327 commitment_train: 0.097327\n",
            "Train Epoch: 2 [16640/60032 (27%)]   time: 0.18   mse_train: 0.066206 vq_train: 0.098299 commitment_train: 0.098299\n",
            "Train Epoch: 2 [17280/60032 (28%)]   time: 0.21   mse_train: 0.067082 vq_train: 0.099252 commitment_train: 0.099252\n",
            "Train Epoch: 2 [17920/60032 (29%)]   time: 0.20   mse_train: 0.064673 vq_train: 0.100626 commitment_train: 0.100626\n",
            "Train Epoch: 2 [18560/60032 (30%)]   time: 0.19   mse_train: 0.064270 vq_train: 0.101364 commitment_train: 0.101364\n",
            "Train Epoch: 2 [19200/60032 (31%)]   time: 0.21   mse_train: 0.064816 vq_train: 0.101813 commitment_train: 0.101813\n",
            "Train Epoch: 2 [19840/60032 (33%)]   time: 0.18   mse_train: 0.067359 vq_train: 0.103555 commitment_train: 0.103555\n",
            "Train Epoch: 2 [20480/60032 (34%)]   time: 0.24   mse_train: 0.066295 vq_train: 0.104189 commitment_train: 0.104189\n",
            "Train Epoch: 2 [21120/60032 (35%)]   time: 0.18   mse_train: 0.068151 vq_train: 0.105290 commitment_train: 0.105290\n",
            "Train Epoch: 2 [21760/60032 (36%)]   time: 0.21   mse_train: 0.063083 vq_train: 0.105962 commitment_train: 0.105962\n",
            "Train Epoch: 2 [22400/60032 (37%)]   time: 0.20   mse_train: 0.064985 vq_train: 0.107273 commitment_train: 0.107273\n",
            "Train Epoch: 2 [23040/60032 (38%)]   time: 0.19   mse_train: 0.064862 vq_train: 0.108361 commitment_train: 0.108361\n",
            "Train Epoch: 2 [23680/60032 (39%)]   time: 0.22   mse_train: 0.064706 vq_train: 0.109329 commitment_train: 0.109329\n",
            "Train Epoch: 2 [24320/60032 (40%)]   time: 0.23   mse_train: 0.063923 vq_train: 0.109685 commitment_train: 0.109685\n",
            "Train Epoch: 2 [24960/60032 (41%)]   time: 0.26   mse_train: 0.064185 vq_train: 0.111460 commitment_train: 0.111460\n",
            "Train Epoch: 2 [25600/60032 (42%)]   time: 0.31   mse_train: 0.063926 vq_train: 0.111872 commitment_train: 0.111872\n",
            "Train Epoch: 2 [26240/60032 (43%)]   time: 0.26   mse_train: 0.061209 vq_train: 0.112483 commitment_train: 0.112483\n",
            "Train Epoch: 2 [26880/60032 (44%)]   time: 0.28   mse_train: 0.061557 vq_train: 0.113605 commitment_train: 0.113605\n",
            "Train Epoch: 2 [27520/60032 (45%)]   time: 0.28   mse_train: 0.062556 vq_train: 0.114898 commitment_train: 0.114898\n",
            "Train Epoch: 2 [28160/60032 (46%)]   time: 0.29   mse_train: 0.063709 vq_train: 0.115913 commitment_train: 0.115913\n",
            "Train Epoch: 2 [28800/60032 (47%)]   time: 0.29   mse_train: 0.063573 vq_train: 0.116921 commitment_train: 0.116921\n",
            "Train Epoch: 2 [29440/60032 (49%)]   time: 0.27   mse_train: 0.062413 vq_train: 0.117677 commitment_train: 0.117677\n",
            "Train Epoch: 2 [30080/60032 (50%)]   time: 0.29   mse_train: 0.062944 vq_train: 0.118627 commitment_train: 0.118627\n",
            "Train Epoch: 2 [30720/60032 (51%)]   time: 0.31   mse_train: 0.062998 vq_train: 0.119887 commitment_train: 0.119887\n",
            "Train Epoch: 2 [31360/60032 (52%)]   time: 0.29   mse_train: 0.062548 vq_train: 0.120486 commitment_train: 0.120486\n",
            "Train Epoch: 2 [32000/60032 (53%)]   time: 0.32   mse_train: 0.063111 vq_train: 0.122290 commitment_train: 0.122290\n",
            "Train Epoch: 2 [32640/60032 (54%)]   time: 0.25   mse_train: 0.064798 vq_train: 0.123189 commitment_train: 0.123189\n",
            "Train Epoch: 2 [33280/60032 (55%)]   time: 0.33   mse_train: 0.062034 vq_train: 0.124324 commitment_train: 0.124324\n",
            "Train Epoch: 2 [33920/60032 (56%)]   time: 0.29   mse_train: 0.065095 vq_train: 0.125538 commitment_train: 0.125538\n",
            "Train Epoch: 2 [34560/60032 (57%)]   time: 0.29   mse_train: 0.063772 vq_train: 0.126368 commitment_train: 0.126368\n",
            "Train Epoch: 2 [35200/60032 (58%)]   time: 0.22   mse_train: 0.061500 vq_train: 0.126867 commitment_train: 0.126867\n",
            "Train Epoch: 2 [35840/60032 (59%)]   time: 0.32   mse_train: 0.064378 vq_train: 0.128809 commitment_train: 0.128809\n",
            "Train Epoch: 2 [36480/60032 (60%)]   time: 0.30   mse_train: 0.062107 vq_train: 0.129847 commitment_train: 0.129847\n",
            "Train Epoch: 2 [37120/60032 (61%)]   time: 0.26   mse_train: 0.061289 vq_train: 0.130208 commitment_train: 0.130208\n",
            "Train Epoch: 2 [37760/60032 (62%)]   time: 0.32   mse_train: 0.061691 vq_train: 0.131225 commitment_train: 0.131225\n",
            "Train Epoch: 2 [38400/60032 (63%)]   time: 0.25   mse_train: 0.061751 vq_train: 0.132784 commitment_train: 0.132784\n",
            "Train Epoch: 2 [39040/60032 (65%)]   time: 0.19   mse_train: 0.061865 vq_train: 0.133646 commitment_train: 0.133646\n",
            "Train Epoch: 2 [39680/60032 (66%)]   time: 0.19   mse_train: 0.062895 vq_train: 0.135176 commitment_train: 0.135176\n",
            "Train Epoch: 2 [40320/60032 (67%)]   time: 0.21   mse_train: 0.062920 vq_train: 0.136244 commitment_train: 0.136244\n",
            "Train Epoch: 2 [40960/60032 (68%)]   time: 0.22   mse_train: 0.062131 vq_train: 0.137324 commitment_train: 0.137324\n",
            "Train Epoch: 2 [41600/60032 (69%)]   time: 0.20   mse_train: 0.059932 vq_train: 0.138450 commitment_train: 0.138450\n",
            "Train Epoch: 2 [42240/60032 (70%)]   time: 0.19   mse_train: 0.064685 vq_train: 0.139286 commitment_train: 0.139286\n",
            "Train Epoch: 2 [42880/60032 (71%)]   time: 0.20   mse_train: 0.062679 vq_train: 0.140676 commitment_train: 0.140676\n",
            "Train Epoch: 2 [43520/60032 (72%)]   time: 0.20   mse_train: 0.061479 vq_train: 0.140954 commitment_train: 0.140954\n",
            "Train Epoch: 2 [44160/60032 (73%)]   time: 0.22   mse_train: 0.062941 vq_train: 0.143432 commitment_train: 0.143432\n",
            "Train Epoch: 2 [44800/60032 (74%)]   time: 0.20   mse_train: 0.061056 vq_train: 0.143991 commitment_train: 0.143991\n",
            "Train Epoch: 2 [45440/60032 (75%)]   time: 0.20   mse_train: 0.060393 vq_train: 0.145065 commitment_train: 0.145065\n",
            "Train Epoch: 2 [46080/60032 (76%)]   time: 0.19   mse_train: 0.058588 vq_train: 0.145275 commitment_train: 0.145275\n",
            "Train Epoch: 2 [46720/60032 (77%)]   time: 0.20   mse_train: 0.059302 vq_train: 0.147000 commitment_train: 0.147000\n",
            "Train Epoch: 2 [47360/60032 (78%)]   time: 0.21   mse_train: 0.059769 vq_train: 0.147915 commitment_train: 0.147915\n",
            "Train Epoch: 2 [48000/60032 (79%)]   time: 0.19   mse_train: 0.060567 vq_train: 0.148932 commitment_train: 0.148932\n",
            "Train Epoch: 2 [48640/60032 (81%)]   time: 0.18   mse_train: 0.062202 vq_train: 0.150389 commitment_train: 0.150389\n",
            "Train Epoch: 2 [49280/60032 (82%)]   time: 0.21   mse_train: 0.058732 vq_train: 0.150422 commitment_train: 0.150422\n",
            "Train Epoch: 2 [49920/60032 (83%)]   time: 0.19   mse_train: 0.059793 vq_train: 0.152361 commitment_train: 0.152361\n",
            "Train Epoch: 2 [50560/60032 (84%)]   time: 0.22   mse_train: 0.058389 vq_train: 0.152679 commitment_train: 0.152679\n",
            "Train Epoch: 2 [51200/60032 (85%)]   time: 0.20   mse_train: 0.056646 vq_train: 0.154126 commitment_train: 0.154126\n",
            "Train Epoch: 2 [51840/60032 (86%)]   time: 0.18   mse_train: 0.059985 vq_train: 0.155587 commitment_train: 0.155587\n",
            "Train Epoch: 2 [52480/60032 (87%)]   time: 0.20   mse_train: 0.060670 vq_train: 0.156659 commitment_train: 0.156659\n",
            "Train Epoch: 2 [53120/60032 (88%)]   time: 0.20   mse_train: 0.060013 vq_train: 0.157421 commitment_train: 0.157421\n",
            "Train Epoch: 2 [53760/60032 (89%)]   time: 0.20   mse_train: 0.057439 vq_train: 0.158491 commitment_train: 0.158491\n",
            "Train Epoch: 2 [54400/60032 (90%)]   time: 0.19   mse_train: 0.058605 vq_train: 0.159700 commitment_train: 0.159700\n",
            "Train Epoch: 2 [55040/60032 (91%)]   time: 0.19   mse_train: 0.060639 vq_train: 0.161249 commitment_train: 0.161249\n",
            "Train Epoch: 2 [55680/60032 (92%)]   time: 0.20   mse_train: 0.059293 vq_train: 0.162200 commitment_train: 0.162200\n",
            "Train Epoch: 2 [56320/60032 (93%)]   time: 0.19   mse_train: 0.059043 vq_train: 0.163102 commitment_train: 0.163102\n",
            "Train Epoch: 2 [56960/60032 (94%)]   time: 0.21   mse_train: 0.057135 vq_train: 0.163478 commitment_train: 0.163478\n",
            "Train Epoch: 2 [57600/60032 (95%)]   time: 0.20   mse_train: 0.059109 vq_train: 0.165585 commitment_train: 0.165585\n",
            "Train Epoch: 2 [58240/60032 (97%)]   time: 0.19   mse_train: 0.058689 vq_train: 0.166509 commitment_train: 0.166509\n",
            "Train Epoch: 2 [58880/60032 (98%)]   time: 0.20   mse_train: 0.061301 vq_train: 0.168077 commitment_train: 0.168077\n",
            "Train Epoch: 2 [59520/60032 (99%)]   time: 0.10   mse_train: 0.060537 vq_train: 0.167782 commitment_train: 0.167782\n",
            "====> Epoch: 2 mse_train: 0.064131\tvq_train: 0.120515\tcommitment_train: 0.120515\n",
            "====> Test set losses: mse_test: 0.057503 vq_test: 0.168896 commitment_test: 0.168896\n",
            "Train Epoch: 3 [    0/60032 ( 0%)]   time: 0.30   mse_train: 0.005799 vq_train: 0.017066 commitment_train: 0.017066\n",
            "Train Epoch: 3 [  640/60032 ( 1%)]   time: 0.48   mse_train: 0.057537 vq_train: 0.170468 commitment_train: 0.170468\n",
            "Train Epoch: 3 [ 1280/60032 ( 2%)]   time: 0.28   mse_train: 0.060009 vq_train: 0.171320 commitment_train: 0.171320\n",
            "Train Epoch: 3 [ 1920/60032 ( 3%)]   time: 0.29   mse_train: 0.058839 vq_train: 0.172397 commitment_train: 0.172397\n",
            "Train Epoch: 3 [ 2560/60032 ( 4%)]   time: 0.32   mse_train: 0.057055 vq_train: 0.173369 commitment_train: 0.173369\n",
            "Train Epoch: 3 [ 3200/60032 ( 5%)]   time: 0.29   mse_train: 0.057530 vq_train: 0.174779 commitment_train: 0.174779\n",
            "Train Epoch: 3 [ 3840/60032 ( 6%)]   time: 0.31   mse_train: 0.057756 vq_train: 0.176026 commitment_train: 0.176026\n",
            "Train Epoch: 3 [ 4480/60032 ( 7%)]   time: 0.35   mse_train: 0.056511 vq_train: 0.176150 commitment_train: 0.176150\n",
            "Train Epoch: 3 [ 5120/60032 ( 8%)]   time: 0.31   mse_train: 0.057375 vq_train: 0.178462 commitment_train: 0.178462\n",
            "Train Epoch: 3 [ 5760/60032 ( 9%)]   time: 0.29   mse_train: 0.056851 vq_train: 0.179086 commitment_train: 0.179086\n",
            "Train Epoch: 3 [ 6400/60032 (10%)]   time: 0.32   mse_train: 0.060473 vq_train: 0.180940 commitment_train: 0.180940\n",
            "Train Epoch: 3 [ 7040/60032 (11%)]   time: 0.31   mse_train: 0.057429 vq_train: 0.180309 commitment_train: 0.180309\n",
            "Train Epoch: 3 [ 7680/60032 (12%)]   time: 0.28   mse_train: 0.056453 vq_train: 0.181637 commitment_train: 0.181637\n",
            "Train Epoch: 3 [ 8320/60032 (13%)]   time: 0.32   mse_train: 0.057921 vq_train: 0.183143 commitment_train: 0.183143\n",
            "Train Epoch: 3 [ 8960/60032 (14%)]   time: 0.29   mse_train: 0.058309 vq_train: 0.184723 commitment_train: 0.184723\n",
            "Train Epoch: 3 [ 9600/60032 (15%)]   time: 0.32   mse_train: 0.057720 vq_train: 0.185224 commitment_train: 0.185224\n",
            "Train Epoch: 3 [10240/60032 (17%)]   time: 0.29   mse_train: 0.056390 vq_train: 0.185762 commitment_train: 0.185762\n",
            "Train Epoch: 3 [10880/60032 (18%)]   time: 0.30   mse_train: 0.056016 vq_train: 0.187224 commitment_train: 0.187224\n",
            "Train Epoch: 3 [11520/60032 (19%)]   time: 0.27   mse_train: 0.059766 vq_train: 0.189653 commitment_train: 0.189653\n",
            "Train Epoch: 3 [12160/60032 (20%)]   time: 0.29   mse_train: 0.058902 vq_train: 0.190489 commitment_train: 0.190489\n",
            "Train Epoch: 3 [12800/60032 (21%)]   time: 0.31   mse_train: 0.058102 vq_train: 0.191062 commitment_train: 0.191062\n",
            "Train Epoch: 3 [13440/60032 (22%)]   time: 0.31   mse_train: 0.055848 vq_train: 0.191448 commitment_train: 0.191448\n",
            "Train Epoch: 3 [14080/60032 (23%)]   time: 0.19   mse_train: 0.055500 vq_train: 0.192968 commitment_train: 0.192968\n",
            "Train Epoch: 3 [14720/60032 (24%)]   time: 0.20   mse_train: 0.058015 vq_train: 0.194425 commitment_train: 0.194425\n",
            "Train Epoch: 3 [15360/60032 (25%)]   time: 0.20   mse_train: 0.058211 vq_train: 0.195753 commitment_train: 0.195753\n",
            "Train Epoch: 3 [16000/60032 (26%)]   time: 0.20   mse_train: 0.055400 vq_train: 0.196730 commitment_train: 0.196730\n",
            "Train Epoch: 3 [16640/60032 (27%)]   time: 0.22   mse_train: 0.055086 vq_train: 0.197516 commitment_train: 0.197516\n",
            "Train Epoch: 3 [17280/60032 (28%)]   time: 0.20   mse_train: 0.056021 vq_train: 0.198398 commitment_train: 0.198398\n",
            "Train Epoch: 3 [17920/60032 (29%)]   time: 0.20   mse_train: 0.057760 vq_train: 0.199651 commitment_train: 0.199651\n",
            "Train Epoch: 3 [18560/60032 (30%)]   time: 0.19   mse_train: 0.055071 vq_train: 0.200910 commitment_train: 0.200910\n",
            "Train Epoch: 3 [19200/60032 (31%)]   time: 0.19   mse_train: 0.056086 vq_train: 0.201225 commitment_train: 0.201225\n",
            "Train Epoch: 3 [19840/60032 (33%)]   time: 0.21   mse_train: 0.055453 vq_train: 0.201921 commitment_train: 0.201921\n",
            "Train Epoch: 3 [20480/60032 (34%)]   time: 0.19   mse_train: 0.056677 vq_train: 0.204092 commitment_train: 0.204092\n",
            "Train Epoch: 3 [21120/60032 (35%)]   time: 0.20   mse_train: 0.057248 vq_train: 0.205188 commitment_train: 0.205188\n",
            "Train Epoch: 3 [21760/60032 (36%)]   time: 0.18   mse_train: 0.056655 vq_train: 0.205065 commitment_train: 0.205065\n",
            "Train Epoch: 3 [22400/60032 (37%)]   time: 0.21   mse_train: 0.054745 vq_train: 0.206382 commitment_train: 0.206382\n",
            "Train Epoch: 3 [23040/60032 (38%)]   time: 0.22   mse_train: 0.053649 vq_train: 0.207120 commitment_train: 0.207120\n",
            "Train Epoch: 3 [23680/60032 (39%)]   time: 0.21   mse_train: 0.056688 vq_train: 0.209460 commitment_train: 0.209460\n",
            "Train Epoch: 3 [24320/60032 (40%)]   time: 0.22   mse_train: 0.056844 vq_train: 0.210425 commitment_train: 0.210425\n",
            "Train Epoch: 3 [24960/60032 (41%)]   time: 0.21   mse_train: 0.054362 vq_train: 0.210376 commitment_train: 0.210376\n",
            "Train Epoch: 3 [25600/60032 (42%)]   time: 0.24   mse_train: 0.057367 vq_train: 0.212301 commitment_train: 0.212301\n",
            "Train Epoch: 3 [26240/60032 (43%)]   time: 0.31   mse_train: 0.054909 vq_train: 0.211724 commitment_train: 0.211724\n",
            "Train Epoch: 3 [26880/60032 (44%)]   time: 0.32   mse_train: 0.056835 vq_train: 0.213451 commitment_train: 0.213451\n",
            "Train Epoch: 3 [27520/60032 (45%)]   time: 0.24   mse_train: 0.055900 vq_train: 0.215613 commitment_train: 0.215613\n",
            "Train Epoch: 3 [28160/60032 (46%)]   time: 0.18   mse_train: 0.055967 vq_train: 0.216014 commitment_train: 0.216014\n",
            "Train Epoch: 3 [28800/60032 (47%)]   time: 0.23   mse_train: 0.055331 vq_train: 0.215932 commitment_train: 0.215932\n",
            "Train Epoch: 3 [29440/60032 (49%)]   time: 0.21   mse_train: 0.054031 vq_train: 0.217748 commitment_train: 0.217748\n",
            "Train Epoch: 3 [30080/60032 (50%)]   time: 0.18   mse_train: 0.056722 vq_train: 0.218562 commitment_train: 0.218562\n",
            "Train Epoch: 3 [30720/60032 (51%)]   time: 0.21   mse_train: 0.053634 vq_train: 0.218556 commitment_train: 0.218556\n",
            "Train Epoch: 3 [31360/60032 (52%)]   time: 0.20   mse_train: 0.056537 vq_train: 0.221597 commitment_train: 0.221597\n",
            "Train Epoch: 3 [32000/60032 (53%)]   time: 0.20   mse_train: 0.053549 vq_train: 0.221639 commitment_train: 0.221639\n",
            "Train Epoch: 3 [32640/60032 (54%)]   time: 0.21   mse_train: 0.054124 vq_train: 0.222106 commitment_train: 0.222106\n",
            "Train Epoch: 3 [33280/60032 (55%)]   time: 0.18   mse_train: 0.056493 vq_train: 0.224417 commitment_train: 0.224417\n",
            "Train Epoch: 3 [33920/60032 (56%)]   time: 0.20   mse_train: 0.054669 vq_train: 0.224776 commitment_train: 0.224776\n",
            "Train Epoch: 3 [34560/60032 (57%)]   time: 0.21   mse_train: 0.054200 vq_train: 0.224554 commitment_train: 0.224554\n",
            "Train Epoch: 3 [35200/60032 (58%)]   time: 0.21   mse_train: 0.055492 vq_train: 0.227119 commitment_train: 0.227119\n",
            "Train Epoch: 3 [35840/60032 (59%)]   time: 0.21   mse_train: 0.054498 vq_train: 0.227936 commitment_train: 0.227936\n",
            "Train Epoch: 3 [36480/60032 (60%)]   time: 0.19   mse_train: 0.054512 vq_train: 0.228389 commitment_train: 0.228389\n",
            "Train Epoch: 3 [37120/60032 (61%)]   time: 0.21   mse_train: 0.055114 vq_train: 0.229275 commitment_train: 0.229275\n",
            "Train Epoch: 3 [37760/60032 (62%)]   time: 0.30   mse_train: 0.054453 vq_train: 0.230679 commitment_train: 0.230679\n",
            "Train Epoch: 3 [38400/60032 (63%)]   time: 0.32   mse_train: 0.053338 vq_train: 0.231717 commitment_train: 0.231717\n",
            "Train Epoch: 3 [39040/60032 (65%)]   time: 0.28   mse_train: 0.055251 vq_train: 0.232653 commitment_train: 0.232653\n",
            "Train Epoch: 3 [39680/60032 (66%)]   time: 0.30   mse_train: 0.052046 vq_train: 0.233446 commitment_train: 0.233446\n",
            "Train Epoch: 3 [40320/60032 (67%)]   time: 0.54   mse_train: 0.054241 vq_train: 0.234557 commitment_train: 0.234557\n",
            "Train Epoch: 3 [40960/60032 (68%)]   time: 0.48   mse_train: 0.054309 vq_train: 0.235317 commitment_train: 0.235317\n",
            "Train Epoch: 3 [41600/60032 (69%)]   time: 0.45   mse_train: 0.055487 vq_train: 0.236423 commitment_train: 0.236423\n",
            "Train Epoch: 3 [42240/60032 (70%)]   time: 0.64   mse_train: 0.055859 vq_train: 0.238202 commitment_train: 0.238202\n",
            "Train Epoch: 3 [42880/60032 (71%)]   time: 0.39   mse_train: 0.056293 vq_train: 0.238546 commitment_train: 0.238546\n",
            "Train Epoch: 3 [43520/60032 (72%)]   time: 0.63   mse_train: 0.053405 vq_train: 0.239830 commitment_train: 0.239830\n",
            "Train Epoch: 3 [44160/60032 (73%)]   time: 0.43   mse_train: 0.053782 vq_train: 0.240943 commitment_train: 0.240943\n",
            "Train Epoch: 3 [44800/60032 (74%)]   time: 0.31   mse_train: 0.051709 vq_train: 0.240545 commitment_train: 0.240545\n",
            "Train Epoch: 3 [45440/60032 (75%)]   time: 0.34   mse_train: 0.052421 vq_train: 0.241045 commitment_train: 0.241045\n",
            "Train Epoch: 3 [46080/60032 (76%)]   time: 0.35   mse_train: 0.053171 vq_train: 0.243095 commitment_train: 0.243095\n",
            "Train Epoch: 3 [46720/60032 (77%)]   time: 0.30   mse_train: 0.054667 vq_train: 0.244124 commitment_train: 0.244124\n",
            "Train Epoch: 3 [47360/60032 (78%)]   time: 0.38   mse_train: 0.055561 vq_train: 0.245493 commitment_train: 0.245493\n",
            "Train Epoch: 3 [48000/60032 (79%)]   time: 0.37   mse_train: 0.054941 vq_train: 0.246680 commitment_train: 0.246680\n",
            "Train Epoch: 3 [48640/60032 (81%)]   time: 0.30   mse_train: 0.053544 vq_train: 0.247734 commitment_train: 0.247734\n",
            "Train Epoch: 3 [49280/60032 (82%)]   time: 0.33   mse_train: 0.056675 vq_train: 0.248528 commitment_train: 0.248528\n",
            "Train Epoch: 3 [49920/60032 (83%)]   time: 0.35   mse_train: 0.051278 vq_train: 0.248144 commitment_train: 0.248144\n",
            "Train Epoch: 3 [50560/60032 (84%)]   time: 0.34   mse_train: 0.052768 vq_train: 0.249768 commitment_train: 0.249768\n",
            "Train Epoch: 3 [51200/60032 (85%)]   time: 0.36   mse_train: 0.054439 vq_train: 0.251645 commitment_train: 0.251645\n",
            "Train Epoch: 3 [51840/60032 (86%)]   time: 0.30   mse_train: 0.051843 vq_train: 0.250711 commitment_train: 0.250711\n",
            "Train Epoch: 3 [52480/60032 (87%)]   time: 0.36   mse_train: 0.052057 vq_train: 0.252427 commitment_train: 0.252427\n",
            "Train Epoch: 3 [53120/60032 (88%)]   time: 0.28   mse_train: 0.054334 vq_train: 0.253598 commitment_train: 0.253598\n",
            "Train Epoch: 3 [53760/60032 (89%)]   time: 0.30   mse_train: 0.053316 vq_train: 0.253247 commitment_train: 0.253247\n",
            "Train Epoch: 3 [54400/60032 (90%)]   time: 0.31   mse_train: 0.053296 vq_train: 0.255658 commitment_train: 0.255658\n",
            "Train Epoch: 3 [55040/60032 (91%)]   time: 0.30   mse_train: 0.055490 vq_train: 0.256937 commitment_train: 0.256937\n",
            "Train Epoch: 3 [55680/60032 (92%)]   time: 0.31   mse_train: 0.053207 vq_train: 0.258921 commitment_train: 0.258921\n",
            "Train Epoch: 3 [56320/60032 (93%)]   time: 0.27   mse_train: 0.053523 vq_train: 0.257606 commitment_train: 0.257606\n",
            "Train Epoch: 3 [56960/60032 (94%)]   time: 0.30   mse_train: 0.055003 vq_train: 0.258851 commitment_train: 0.258851\n",
            "Train Epoch: 3 [57600/60032 (95%)]   time: 0.29   mse_train: 0.054182 vq_train: 0.260162 commitment_train: 0.260162\n",
            "Train Epoch: 3 [58240/60032 (97%)]   time: 0.21   mse_train: 0.053004 vq_train: 0.261319 commitment_train: 0.261319\n",
            "Train Epoch: 3 [58880/60032 (98%)]   time: 0.19   mse_train: 0.053120 vq_train: 0.262575 commitment_train: 0.262575\n",
            "Train Epoch: 3 [59520/60032 (99%)]   time: 0.12   mse_train: 0.053307 vq_train: 0.263746 commitment_train: 0.263746\n",
            "====> Epoch: 3 mse_train: 0.055449\tvq_train: 0.218435\tcommitment_train: 0.218435\n",
            "====> Test set losses: mse_test: 0.052758 vq_test: 0.274847 commitment_test: 0.274847\n",
            "Train Epoch: 4 [    0/60032 ( 0%)]   time: 0.29   mse_train: 0.005199 vq_train: 0.026187 commitment_train: 0.026187\n",
            "Train Epoch: 4 [  640/60032 ( 1%)]   time: 0.36   mse_train: 0.053445 vq_train: 0.264902 commitment_train: 0.264902\n",
            "Train Epoch: 4 [ 1280/60032 ( 2%)]   time: 0.21   mse_train: 0.052620 vq_train: 0.264642 commitment_train: 0.264642\n",
            "Train Epoch: 4 [ 1920/60032 ( 3%)]   time: 0.23   mse_train: 0.054617 vq_train: 0.267437 commitment_train: 0.267437\n",
            "Train Epoch: 4 [ 2560/60032 ( 4%)]   time: 0.20   mse_train: 0.054695 vq_train: 0.267672 commitment_train: 0.267672\n",
            "Train Epoch: 4 [ 3200/60032 ( 5%)]   time: 0.21   mse_train: 0.053003 vq_train: 0.267911 commitment_train: 0.267911\n",
            "Train Epoch: 4 [ 3840/60032 ( 6%)]   time: 0.19   mse_train: 0.054407 vq_train: 0.270138 commitment_train: 0.270138\n",
            "Train Epoch: 4 [ 4480/60032 ( 7%)]   time: 0.24   mse_train: 0.054009 vq_train: 0.269694 commitment_train: 0.269694\n",
            "Train Epoch: 4 [ 5120/60032 ( 8%)]   time: 0.20   mse_train: 0.051201 vq_train: 0.269906 commitment_train: 0.269906\n",
            "Train Epoch: 4 [ 5760/60032 ( 9%)]   time: 0.21   mse_train: 0.053703 vq_train: 0.271697 commitment_train: 0.271697\n",
            "Train Epoch: 4 [ 6400/60032 (10%)]   time: 0.21   mse_train: 0.052001 vq_train: 0.272293 commitment_train: 0.272293\n",
            "Train Epoch: 4 [ 7040/60032 (11%)]   time: 0.22   mse_train: 0.052204 vq_train: 0.272817 commitment_train: 0.272817\n",
            "Train Epoch: 4 [ 7680/60032 (12%)]   time: 0.22   mse_train: 0.052965 vq_train: 0.274127 commitment_train: 0.274127\n",
            "Train Epoch: 4 [ 8320/60032 (13%)]   time: 0.19   mse_train: 0.051789 vq_train: 0.273603 commitment_train: 0.273603\n",
            "Train Epoch: 4 [ 8960/60032 (14%)]   time: 0.21   mse_train: 0.051911 vq_train: 0.275638 commitment_train: 0.275638\n",
            "Train Epoch: 4 [ 9600/60032 (15%)]   time: 0.21   mse_train: 0.053932 vq_train: 0.278521 commitment_train: 0.278521\n",
            "Train Epoch: 4 [10240/60032 (17%)]   time: 0.19   mse_train: 0.054047 vq_train: 0.277598 commitment_train: 0.277598\n",
            "Train Epoch: 4 [10880/60032 (18%)]   time: 0.22   mse_train: 0.051726 vq_train: 0.277451 commitment_train: 0.277451\n",
            "Train Epoch: 4 [11520/60032 (19%)]   time: 0.20   mse_train: 0.053773 vq_train: 0.280944 commitment_train: 0.280944\n",
            "Train Epoch: 4 [12160/60032 (20%)]   time: 0.19   mse_train: 0.053291 vq_train: 0.280970 commitment_train: 0.280970\n",
            "Train Epoch: 4 [12800/60032 (21%)]   time: 0.19   mse_train: 0.052217 vq_train: 0.281749 commitment_train: 0.281749\n",
            "Train Epoch: 4 [13440/60032 (22%)]   time: 0.31   mse_train: 0.053210 vq_train: 0.281468 commitment_train: 0.281468\n",
            "Train Epoch: 4 [14080/60032 (23%)]   time: 0.23   mse_train: 0.051571 vq_train: 0.283977 commitment_train: 0.283977\n",
            "Train Epoch: 4 [14720/60032 (24%)]   time: 0.19   mse_train: 0.053515 vq_train: 0.283260 commitment_train: 0.283260\n",
            "Train Epoch: 4 [15360/60032 (25%)]   time: 0.21   mse_train: 0.051155 vq_train: 0.283853 commitment_train: 0.283853\n",
            "Train Epoch: 4 [16000/60032 (26%)]   time: 0.20   mse_train: 0.053513 vq_train: 0.286674 commitment_train: 0.286674\n",
            "Train Epoch: 4 [16640/60032 (27%)]   time: 0.20   mse_train: 0.052115 vq_train: 0.285398 commitment_train: 0.285398\n",
            "Train Epoch: 4 [17280/60032 (28%)]   time: 0.21   mse_train: 0.051239 vq_train: 0.286744 commitment_train: 0.286744\n",
            "Train Epoch: 4 [17920/60032 (29%)]   time: 0.20   mse_train: 0.051950 vq_train: 0.288468 commitment_train: 0.288468\n",
            "Train Epoch: 4 [18560/60032 (30%)]   time: 0.27   mse_train: 0.054133 vq_train: 0.289191 commitment_train: 0.289191\n",
            "Train Epoch: 4 [19200/60032 (31%)]   time: 0.30   mse_train: 0.051427 vq_train: 0.288746 commitment_train: 0.288746\n",
            "Train Epoch: 4 [19840/60032 (33%)]   time: 0.32   mse_train: 0.054781 vq_train: 0.291076 commitment_train: 0.291076\n",
            "Train Epoch: 4 [20480/60032 (34%)]   time: 0.29   mse_train: 0.051915 vq_train: 0.290932 commitment_train: 0.290932\n",
            "Train Epoch: 4 [21120/60032 (35%)]   time: 0.33   mse_train: 0.050728 vq_train: 0.291335 commitment_train: 0.291335\n",
            "Train Epoch: 4 [21760/60032 (36%)]   time: 0.30   mse_train: 0.052633 vq_train: 0.292152 commitment_train: 0.292152\n",
            "Train Epoch: 4 [22400/60032 (37%)]   time: 0.31   mse_train: 0.051421 vq_train: 0.292406 commitment_train: 0.292406\n",
            "Train Epoch: 4 [23040/60032 (38%)]   time: 0.27   mse_train: 0.049566 vq_train: 0.292400 commitment_train: 0.292400\n",
            "Train Epoch: 4 [23680/60032 (39%)]   time: 0.30   mse_train: 0.053520 vq_train: 0.296277 commitment_train: 0.296277\n",
            "Train Epoch: 4 [24320/60032 (40%)]   time: 0.30   mse_train: 0.050622 vq_train: 0.295251 commitment_train: 0.295251\n",
            "Train Epoch: 4 [24960/60032 (41%)]   time: 0.29   mse_train: 0.050916 vq_train: 0.295634 commitment_train: 0.295634\n",
            "Train Epoch: 4 [25600/60032 (42%)]   time: 0.32   mse_train: 0.051787 vq_train: 0.297666 commitment_train: 0.297666\n",
            "Train Epoch: 4 [26240/60032 (43%)]   time: 0.27   mse_train: 0.051200 vq_train: 0.296691 commitment_train: 0.296691\n",
            "Train Epoch: 4 [26880/60032 (44%)]   time: 0.28   mse_train: 0.053341 vq_train: 0.299122 commitment_train: 0.299122\n",
            "Train Epoch: 4 [27520/60032 (45%)]   time: 0.29   mse_train: 0.049743 vq_train: 0.297946 commitment_train: 0.297946\n",
            "Train Epoch: 4 [28160/60032 (46%)]   time: 0.27   mse_train: 0.052742 vq_train: 0.301255 commitment_train: 0.301255\n",
            "Train Epoch: 4 [28800/60032 (47%)]   time: 0.29   mse_train: 0.051193 vq_train: 0.299696 commitment_train: 0.299696\n",
            "Train Epoch: 4 [29440/60032 (49%)]   time: 0.30   mse_train: 0.050888 vq_train: 0.301151 commitment_train: 0.301151\n",
            "Train Epoch: 4 [30080/60032 (50%)]   time: 0.29   mse_train: 0.052213 vq_train: 0.303061 commitment_train: 0.303061\n",
            "Train Epoch: 4 [30720/60032 (51%)]   time: 0.30   mse_train: 0.050967 vq_train: 0.301609 commitment_train: 0.301609\n",
            "Train Epoch: 4 [31360/60032 (52%)]   time: 0.31   mse_train: 0.051962 vq_train: 0.302660 commitment_train: 0.302660\n",
            "Train Epoch: 4 [32000/60032 (53%)]   time: 0.23   mse_train: 0.052507 vq_train: 0.304479 commitment_train: 0.304479\n",
            "Train Epoch: 4 [32640/60032 (54%)]   time: 0.20   mse_train: 0.050371 vq_train: 0.304454 commitment_train: 0.304454\n",
            "Train Epoch: 4 [33280/60032 (55%)]   time: 0.20   mse_train: 0.052140 vq_train: 0.303961 commitment_train: 0.303961\n",
            "Train Epoch: 4 [33920/60032 (56%)]   time: 0.21   mse_train: 0.051348 vq_train: 0.305587 commitment_train: 0.305587\n",
            "Train Epoch: 4 [34560/60032 (57%)]   time: 0.20   mse_train: 0.052727 vq_train: 0.305592 commitment_train: 0.305592\n",
            "Train Epoch: 4 [35200/60032 (58%)]   time: 0.21   mse_train: 0.051247 vq_train: 0.305170 commitment_train: 0.305170\n",
            "Train Epoch: 4 [35840/60032 (59%)]   time: 0.21   mse_train: 0.050217 vq_train: 0.305897 commitment_train: 0.305897\n",
            "Train Epoch: 4 [36480/60032 (60%)]   time: 0.17   mse_train: 0.049386 vq_train: 0.305208 commitment_train: 0.305208\n",
            "Train Epoch: 4 [37120/60032 (61%)]   time: 0.21   mse_train: 0.051319 vq_train: 0.306838 commitment_train: 0.306838\n",
            "Train Epoch: 4 [37760/60032 (62%)]   time: 0.23   mse_train: 0.051909 vq_train: 0.309681 commitment_train: 0.309681\n",
            "Train Epoch: 4 [38400/60032 (63%)]   time: 0.19   mse_train: 0.048928 vq_train: 0.306150 commitment_train: 0.306150\n",
            "Train Epoch: 4 [39040/60032 (65%)]   time: 0.22   mse_train: 0.050100 vq_train: 0.308405 commitment_train: 0.308405\n",
            "Train Epoch: 4 [39680/60032 (66%)]   time: 0.20   mse_train: 0.050420 vq_train: 0.308786 commitment_train: 0.308786\n",
            "Train Epoch: 4 [40320/60032 (67%)]   time: 0.20   mse_train: 0.051000 vq_train: 0.308382 commitment_train: 0.308382\n",
            "Train Epoch: 4 [40960/60032 (68%)]   time: 0.20   mse_train: 0.051680 vq_train: 0.310407 commitment_train: 0.310407\n",
            "Train Epoch: 4 [41600/60032 (69%)]   time: 0.21   mse_train: 0.050205 vq_train: 0.309205 commitment_train: 0.309205\n",
            "Train Epoch: 4 [42240/60032 (70%)]   time: 0.19   mse_train: 0.051226 vq_train: 0.310286 commitment_train: 0.310286\n",
            "Train Epoch: 4 [42880/60032 (71%)]   time: 0.20   mse_train: 0.049890 vq_train: 0.312323 commitment_train: 0.312323\n",
            "Train Epoch: 4 [43520/60032 (72%)]   time: 0.20   mse_train: 0.049772 vq_train: 0.311739 commitment_train: 0.311739\n",
            "Train Epoch: 4 [44160/60032 (73%)]   time: 0.19   mse_train: 0.050371 vq_train: 0.313465 commitment_train: 0.313465\n",
            "Train Epoch: 4 [44800/60032 (74%)]   time: 0.20   mse_train: 0.052151 vq_train: 0.314326 commitment_train: 0.314326\n",
            "Train Epoch: 4 [45440/60032 (75%)]   time: 0.21   mse_train: 0.050561 vq_train: 0.314791 commitment_train: 0.314791\n",
            "Train Epoch: 4 [46080/60032 (76%)]   time: 0.18   mse_train: 0.053315 vq_train: 0.315681 commitment_train: 0.315681\n",
            "Train Epoch: 4 [46720/60032 (77%)]   time: 0.20   mse_train: 0.051972 vq_train: 0.315157 commitment_train: 0.315157\n",
            "Train Epoch: 4 [47360/60032 (78%)]   time: 0.22   mse_train: 0.050680 vq_train: 0.315614 commitment_train: 0.315614\n",
            "Train Epoch: 4 [48000/60032 (79%)]   time: 0.20   mse_train: 0.050427 vq_train: 0.315235 commitment_train: 0.315235\n",
            "Train Epoch: 4 [48640/60032 (81%)]   time: 0.19   mse_train: 0.049244 vq_train: 0.315583 commitment_train: 0.315583\n",
            "Train Epoch: 4 [49280/60032 (82%)]   time: 0.19   mse_train: 0.048577 vq_train: 0.313812 commitment_train: 0.313812\n",
            "Train Epoch: 4 [49920/60032 (83%)]   time: 0.23   mse_train: 0.052262 vq_train: 0.318509 commitment_train: 0.318509\n",
            "Train Epoch: 4 [50560/60032 (84%)]   time: 0.19   mse_train: 0.049467 vq_train: 0.316811 commitment_train: 0.316811\n",
            "Train Epoch: 4 [51200/60032 (85%)]   time: 0.22   mse_train: 0.049650 vq_train: 0.317508 commitment_train: 0.317508\n",
            "Train Epoch: 4 [51840/60032 (86%)]   time: 0.20   mse_train: 0.050227 vq_train: 0.318240 commitment_train: 0.318240\n",
            "Train Epoch: 4 [52480/60032 (87%)]   time: 0.22   mse_train: 0.049850 vq_train: 0.315952 commitment_train: 0.315952\n",
            "Train Epoch: 4 [53120/60032 (88%)]   time: 0.20   mse_train: 0.050164 vq_train: 0.318677 commitment_train: 0.318677\n",
            "Train Epoch: 4 [53760/60032 (89%)]   time: 0.22   mse_train: 0.050071 vq_train: 0.316847 commitment_train: 0.316847\n",
            "Train Epoch: 4 [54400/60032 (90%)]   time: 0.20   mse_train: 0.049735 vq_train: 0.316478 commitment_train: 0.316478\n",
            "Train Epoch: 4 [55040/60032 (91%)]   time: 0.20   mse_train: 0.049699 vq_train: 0.319790 commitment_train: 0.319790\n",
            "Train Epoch: 4 [55680/60032 (92%)]   time: 0.22   mse_train: 0.050993 vq_train: 0.320595 commitment_train: 0.320595\n",
            "Train Epoch: 4 [56320/60032 (93%)]   time: 0.18   mse_train: 0.050620 vq_train: 0.319218 commitment_train: 0.319218\n",
            "Train Epoch: 4 [56960/60032 (94%)]   time: 0.20   mse_train: 0.050090 vq_train: 0.319192 commitment_train: 0.319192\n",
            "Train Epoch: 4 [57600/60032 (95%)]   time: 0.22   mse_train: 0.050463 vq_train: 0.319860 commitment_train: 0.319860\n",
            "Train Epoch: 4 [58240/60032 (97%)]   time: 0.20   mse_train: 0.049646 vq_train: 0.317988 commitment_train: 0.317988\n",
            "Train Epoch: 4 [58880/60032 (98%)]   time: 0.22   mse_train: 0.051867 vq_train: 0.320863 commitment_train: 0.320863\n",
            "Train Epoch: 4 [59520/60032 (99%)]   time: 0.10   mse_train: 0.051177 vq_train: 0.321547 commitment_train: 0.321547\n",
            "====> Epoch: 4 mse_train: 0.051601\tvq_train: 0.298439\tcommitment_train: 0.298439\n",
            "====> Test set losses: mse_test: 0.049792 vq_test: 0.319607 commitment_test: 0.319607\n",
            "Train Epoch: 5 [    0/60032 ( 0%)]   time: 0.57   mse_train: 0.005033 vq_train: 0.031705 commitment_train: 0.031705\n",
            "Train Epoch: 5 [  640/60032 ( 1%)]   time: 0.45   mse_train: 0.050230 vq_train: 0.320492 commitment_train: 0.320492\n",
            "Train Epoch: 5 [ 1280/60032 ( 2%)]   time: 0.30   mse_train: 0.048904 vq_train: 0.321222 commitment_train: 0.321222\n",
            "Train Epoch: 5 [ 1920/60032 ( 3%)]   time: 0.30   mse_train: 0.050909 vq_train: 0.322761 commitment_train: 0.322761\n",
            "Train Epoch: 5 [ 2560/60032 ( 4%)]   time: 0.30   mse_train: 0.048079 vq_train: 0.317768 commitment_train: 0.317768\n",
            "Train Epoch: 5 [ 3200/60032 ( 5%)]   time: 0.29   mse_train: 0.048616 vq_train: 0.320677 commitment_train: 0.320677\n",
            "Train Epoch: 5 [ 3840/60032 ( 6%)]   time: 0.25   mse_train: 0.049646 vq_train: 0.320249 commitment_train: 0.320249\n",
            "Train Epoch: 5 [ 4480/60032 ( 7%)]   time: 0.19   mse_train: 0.048845 vq_train: 0.321427 commitment_train: 0.321427\n",
            "Train Epoch: 5 [ 5120/60032 ( 8%)]   time: 0.21   mse_train: 0.050043 vq_train: 0.321576 commitment_train: 0.321576\n",
            "Train Epoch: 5 [ 5760/60032 ( 9%)]   time: 0.20   mse_train: 0.049023 vq_train: 0.320111 commitment_train: 0.320111\n",
            "Train Epoch: 5 [ 6400/60032 (10%)]   time: 0.22   mse_train: 0.051394 vq_train: 0.322616 commitment_train: 0.322616\n",
            "Train Epoch: 5 [ 7040/60032 (11%)]   time: 0.20   mse_train: 0.049237 vq_train: 0.321698 commitment_train: 0.321698\n",
            "Train Epoch: 5 [ 7680/60032 (12%)]   time: 0.21   mse_train: 0.049663 vq_train: 0.323548 commitment_train: 0.323548\n",
            "Train Epoch: 5 [ 8320/60032 (13%)]   time: 0.19   mse_train: 0.050055 vq_train: 0.320679 commitment_train: 0.320679\n",
            "Train Epoch: 5 [ 8960/60032 (14%)]   time: 0.24   mse_train: 0.049081 vq_train: 0.320956 commitment_train: 0.320956\n",
            "Train Epoch: 5 [ 9600/60032 (15%)]   time: 0.22   mse_train: 0.049269 vq_train: 0.320739 commitment_train: 0.320739\n",
            "Train Epoch: 5 [10240/60032 (17%)]   time: 0.20   mse_train: 0.049758 vq_train: 0.320707 commitment_train: 0.320707\n",
            "Train Epoch: 5 [10880/60032 (18%)]   time: 0.20   mse_train: 0.050217 vq_train: 0.324457 commitment_train: 0.324457\n",
            "Train Epoch: 5 [11520/60032 (19%)]   time: 0.20   mse_train: 0.049876 vq_train: 0.320099 commitment_train: 0.320099\n",
            "Train Epoch: 5 [12160/60032 (20%)]   time: 0.21   mse_train: 0.048970 vq_train: 0.323099 commitment_train: 0.323099\n",
            "Train Epoch: 5 [12800/60032 (21%)]   time: 0.22   mse_train: 0.048168 vq_train: 0.322047 commitment_train: 0.322047\n",
            "Train Epoch: 5 [13440/60032 (22%)]   time: 0.19   mse_train: 0.046891 vq_train: 0.320829 commitment_train: 0.320829\n",
            "Train Epoch: 5 [14080/60032 (23%)]   time: 0.21   mse_train: 0.048772 vq_train: 0.322732 commitment_train: 0.322732\n",
            "Train Epoch: 5 [14720/60032 (24%)]   time: 0.21   mse_train: 0.048017 vq_train: 0.319695 commitment_train: 0.319695\n",
            "Train Epoch: 5 [15360/60032 (25%)]   time: 0.22   mse_train: 0.049411 vq_train: 0.324578 commitment_train: 0.324578\n",
            "Train Epoch: 5 [16000/60032 (26%)]   time: 0.22   mse_train: 0.049157 vq_train: 0.321854 commitment_train: 0.321854\n",
            "Train Epoch: 5 [16640/60032 (27%)]   time: 0.21   mse_train: 0.047718 vq_train: 0.320663 commitment_train: 0.320663\n",
            "Train Epoch: 5 [17280/60032 (28%)]   time: 0.20   mse_train: 0.047665 vq_train: 0.319408 commitment_train: 0.319408\n",
            "Train Epoch: 5 [17920/60032 (29%)]   time: 0.24   mse_train: 0.048332 vq_train: 0.322399 commitment_train: 0.322399\n",
            "Train Epoch: 5 [18560/60032 (30%)]   time: 0.23   mse_train: 0.049412 vq_train: 0.321150 commitment_train: 0.321150\n",
            "Train Epoch: 5 [19200/60032 (31%)]   time: 0.20   mse_train: 0.048318 vq_train: 0.322448 commitment_train: 0.322448\n",
            "Train Epoch: 5 [19840/60032 (33%)]   time: 0.18   mse_train: 0.049506 vq_train: 0.323580 commitment_train: 0.323580\n",
            "Train Epoch: 5 [20480/60032 (34%)]   time: 0.21   mse_train: 0.050115 vq_train: 0.322735 commitment_train: 0.322735\n",
            "Train Epoch: 5 [21120/60032 (35%)]   time: 0.21   mse_train: 0.049056 vq_train: 0.322113 commitment_train: 0.322113\n",
            "Train Epoch: 5 [21760/60032 (36%)]   time: 0.19   mse_train: 0.049558 vq_train: 0.323087 commitment_train: 0.323087\n",
            "Train Epoch: 5 [22400/60032 (37%)]   time: 0.22   mse_train: 0.050587 vq_train: 0.324287 commitment_train: 0.324287\n",
            "Train Epoch: 5 [23040/60032 (38%)]   time: 0.20   mse_train: 0.047678 vq_train: 0.321717 commitment_train: 0.321717\n",
            "Train Epoch: 5 [23680/60032 (39%)]   time: 0.20   mse_train: 0.049107 vq_train: 0.321430 commitment_train: 0.321430\n",
            "Train Epoch: 5 [24320/60032 (40%)]   time: 0.27   mse_train: 0.048283 vq_train: 0.325001 commitment_train: 0.325001\n",
            "Train Epoch: 5 [24960/60032 (41%)]   time: 0.27   mse_train: 0.046795 vq_train: 0.319344 commitment_train: 0.319344\n",
            "Train Epoch: 5 [25600/60032 (42%)]   time: 0.19   mse_train: 0.048154 vq_train: 0.323847 commitment_train: 0.323847\n",
            "Train Epoch: 5 [26240/60032 (43%)]   time: 0.20   mse_train: 0.049236 vq_train: 0.320971 commitment_train: 0.320971\n",
            "Train Epoch: 5 [26880/60032 (44%)]   time: 0.22   mse_train: 0.047298 vq_train: 0.320564 commitment_train: 0.320564\n",
            "Train Epoch: 5 [27520/60032 (45%)]   time: 0.21   mse_train: 0.049061 vq_train: 0.322216 commitment_train: 0.322216\n",
            "Train Epoch: 5 [28160/60032 (46%)]   time: 0.21   mse_train: 0.047651 vq_train: 0.323153 commitment_train: 0.323153\n",
            "Train Epoch: 5 [28800/60032 (47%)]   time: 0.21   mse_train: 0.049626 vq_train: 0.321090 commitment_train: 0.321090\n",
            "Train Epoch: 5 [29440/60032 (49%)]   time: 0.20   mse_train: 0.050223 vq_train: 0.323935 commitment_train: 0.323935\n",
            "Train Epoch: 5 [30080/60032 (50%)]   time: 0.22   mse_train: 0.048682 vq_train: 0.322941 commitment_train: 0.322941\n",
            "Train Epoch: 5 [30720/60032 (51%)]   time: 0.19   mse_train: 0.046528 vq_train: 0.321640 commitment_train: 0.321640\n",
            "Train Epoch: 5 [31360/60032 (52%)]   time: 0.23   mse_train: 0.048761 vq_train: 0.321993 commitment_train: 0.321993\n",
            "Train Epoch: 5 [32000/60032 (53%)]   time: 0.20   mse_train: 0.049844 vq_train: 0.325638 commitment_train: 0.325638\n",
            "Train Epoch: 5 [32640/60032 (54%)]   time: 0.20   mse_train: 0.048634 vq_train: 0.321979 commitment_train: 0.321979\n",
            "Train Epoch: 5 [33280/60032 (55%)]   time: 0.24   mse_train: 0.049019 vq_train: 0.324832 commitment_train: 0.324832\n",
            "Train Epoch: 5 [33920/60032 (56%)]   time: 0.29   mse_train: 0.050507 vq_train: 0.324002 commitment_train: 0.324002\n",
            "Train Epoch: 5 [34560/60032 (57%)]   time: 0.29   mse_train: 0.048875 vq_train: 0.323730 commitment_train: 0.323730\n",
            "Train Epoch: 5 [35200/60032 (58%)]   time: 0.32   mse_train: 0.047993 vq_train: 0.321632 commitment_train: 0.321632\n",
            "Train Epoch: 5 [35840/60032 (59%)]   time: 0.33   mse_train: 0.049692 vq_train: 0.323787 commitment_train: 0.323787\n",
            "Train Epoch: 5 [36480/60032 (60%)]   time: 0.33   mse_train: 0.050968 vq_train: 0.327013 commitment_train: 0.327013\n",
            "Train Epoch: 5 [37120/60032 (61%)]   time: 0.26   mse_train: 0.047313 vq_train: 0.323091 commitment_train: 0.323091\n",
            "Train Epoch: 5 [37760/60032 (62%)]   time: 0.31   mse_train: 0.046676 vq_train: 0.321748 commitment_train: 0.321748\n",
            "Train Epoch: 5 [38400/60032 (63%)]   time: 0.28   mse_train: 0.047488 vq_train: 0.322249 commitment_train: 0.322249\n",
            "Train Epoch: 5 [39040/60032 (65%)]   time: 0.33   mse_train: 0.047526 vq_train: 0.322703 commitment_train: 0.322703\n",
            "Train Epoch: 5 [39680/60032 (66%)]   time: 0.29   mse_train: 0.048426 vq_train: 0.323876 commitment_train: 0.323876\n",
            "Train Epoch: 5 [40320/60032 (67%)]   time: 0.34   mse_train: 0.048879 vq_train: 0.320234 commitment_train: 0.320234\n",
            "Train Epoch: 5 [40960/60032 (68%)]   time: 0.29   mse_train: 0.047541 vq_train: 0.322096 commitment_train: 0.322096\n",
            "Train Epoch: 5 [41600/60032 (69%)]   time: 0.31   mse_train: 0.049377 vq_train: 0.320984 commitment_train: 0.320984\n",
            "Train Epoch: 5 [42240/60032 (70%)]   time: 0.26   mse_train: 0.049416 vq_train: 0.325185 commitment_train: 0.325185\n",
            "Train Epoch: 5 [42880/60032 (71%)]   time: 0.30   mse_train: 0.046676 vq_train: 0.316679 commitment_train: 0.316679\n",
            "Train Epoch: 5 [43520/60032 (72%)]   time: 0.30   mse_train: 0.049012 vq_train: 0.320724 commitment_train: 0.320724\n",
            "Train Epoch: 5 [44160/60032 (73%)]   time: 0.32   mse_train: 0.048123 vq_train: 0.318981 commitment_train: 0.318981\n",
            "Train Epoch: 5 [44800/60032 (74%)]   time: 0.28   mse_train: 0.045986 vq_train: 0.319256 commitment_train: 0.319256\n",
            "Train Epoch: 5 [45440/60032 (75%)]   time: 0.33   mse_train: 0.048268 vq_train: 0.320011 commitment_train: 0.320011\n",
            "Train Epoch: 5 [46080/60032 (76%)]   time: 0.27   mse_train: 0.048313 vq_train: 0.319520 commitment_train: 0.319520\n",
            "Train Epoch: 5 [46720/60032 (77%)]   time: 0.29   mse_train: 0.049477 vq_train: 0.319882 commitment_train: 0.319882\n",
            "Train Epoch: 5 [47360/60032 (78%)]   time: 0.21   mse_train: 0.049360 vq_train: 0.321683 commitment_train: 0.321683\n",
            "Train Epoch: 5 [48000/60032 (79%)]   time: 0.22   mse_train: 0.047284 vq_train: 0.319536 commitment_train: 0.319536\n",
            "Train Epoch: 5 [48640/60032 (81%)]   time: 0.20   mse_train: 0.048904 vq_train: 0.321213 commitment_train: 0.321213\n",
            "Train Epoch: 5 [49280/60032 (82%)]   time: 0.20   mse_train: 0.047585 vq_train: 0.315798 commitment_train: 0.315798\n",
            "Train Epoch: 5 [49920/60032 (83%)]   time: 0.19   mse_train: 0.047104 vq_train: 0.319508 commitment_train: 0.319508\n",
            "Train Epoch: 5 [50560/60032 (84%)]   time: 0.23   mse_train: 0.046336 vq_train: 0.316103 commitment_train: 0.316103\n",
            "Train Epoch: 5 [51200/60032 (85%)]   time: 0.21   mse_train: 0.047713 vq_train: 0.317313 commitment_train: 0.317313\n",
            "Train Epoch: 5 [51840/60032 (86%)]   time: 0.20   mse_train: 0.046904 vq_train: 0.316842 commitment_train: 0.316842\n",
            "Train Epoch: 5 [52480/60032 (87%)]   time: 0.20   mse_train: 0.047270 vq_train: 0.318693 commitment_train: 0.318693\n",
            "Train Epoch: 5 [53120/60032 (88%)]   time: 0.20   mse_train: 0.048483 vq_train: 0.317092 commitment_train: 0.317092\n",
            "Train Epoch: 5 [53760/60032 (89%)]   time: 0.24   mse_train: 0.048626 vq_train: 0.317933 commitment_train: 0.317933\n",
            "Train Epoch: 5 [54400/60032 (90%)]   time: 0.21   mse_train: 0.045952 vq_train: 0.316715 commitment_train: 0.316715\n",
            "Train Epoch: 5 [55040/60032 (91%)]   time: 0.19   mse_train: 0.047190 vq_train: 0.317175 commitment_train: 0.317175\n",
            "Train Epoch: 5 [55680/60032 (92%)]   time: 0.20   mse_train: 0.048484 vq_train: 0.319031 commitment_train: 0.319031\n",
            "Train Epoch: 5 [56320/60032 (93%)]   time: 0.20   mse_train: 0.048227 vq_train: 0.315757 commitment_train: 0.315757\n",
            "Train Epoch: 5 [56960/60032 (94%)]   time: 0.21   mse_train: 0.047012 vq_train: 0.315354 commitment_train: 0.315354\n",
            "Train Epoch: 5 [57600/60032 (95%)]   time: 0.20   mse_train: 0.048561 vq_train: 0.319020 commitment_train: 0.319020\n",
            "Train Epoch: 5 [58240/60032 (97%)]   time: 0.21   mse_train: 0.045636 vq_train: 0.314193 commitment_train: 0.314193\n",
            "Train Epoch: 5 [58880/60032 (98%)]   time: 0.19   mse_train: 0.045840 vq_train: 0.312335 commitment_train: 0.312335\n",
            "Train Epoch: 5 [59520/60032 (99%)]   time: 0.11   mse_train: 0.047169 vq_train: 0.316807 commitment_train: 0.316807\n",
            "====> Epoch: 5 mse_train: 0.048542\tvq_train: 0.321042\tcommitment_train: 0.321042\n",
            "====> Test set losses: mse_test: 0.046779 vq_test: 0.322630 commitment_test: 0.322630\n",
            "Train Epoch: 6 [    0/60032 ( 0%)]   time: 0.40   mse_train: 0.005251 vq_train: 0.032151 commitment_train: 0.032151\n",
            "Train Epoch: 6 [  640/60032 ( 1%)]   time: 0.31   mse_train: 0.047029 vq_train: 0.314627 commitment_train: 0.314627\n",
            "Train Epoch: 6 [ 1280/60032 ( 2%)]   time: 0.21   mse_train: 0.048164 vq_train: 0.314237 commitment_train: 0.314237\n",
            "Train Epoch: 6 [ 1920/60032 ( 3%)]   time: 0.20   mse_train: 0.048381 vq_train: 0.313834 commitment_train: 0.313834\n",
            "Train Epoch: 6 [ 2560/60032 ( 4%)]   time: 0.22   mse_train: 0.045923 vq_train: 0.313424 commitment_train: 0.313424\n",
            "Train Epoch: 6 [ 3200/60032 ( 5%)]   time: 0.19   mse_train: 0.047687 vq_train: 0.316896 commitment_train: 0.316896\n",
            "Train Epoch: 6 [ 3840/60032 ( 6%)]   time: 0.20   mse_train: 0.046079 vq_train: 0.310927 commitment_train: 0.310927\n",
            "Train Epoch: 6 [ 4480/60032 ( 7%)]   time: 0.20   mse_train: 0.049338 vq_train: 0.315158 commitment_train: 0.315158\n",
            "Train Epoch: 6 [ 5120/60032 ( 8%)]   time: 0.22   mse_train: 0.047656 vq_train: 0.313467 commitment_train: 0.313467\n",
            "Train Epoch: 6 [ 5760/60032 ( 9%)]   time: 0.24   mse_train: 0.048746 vq_train: 0.314885 commitment_train: 0.314885\n",
            "Train Epoch: 6 [ 6400/60032 (10%)]   time: 0.21   mse_train: 0.049172 vq_train: 0.313052 commitment_train: 0.313052\n",
            "Train Epoch: 6 [ 7040/60032 (11%)]   time: 0.28   mse_train: 0.047432 vq_train: 0.316804 commitment_train: 0.316804\n",
            "Train Epoch: 6 [ 7680/60032 (12%)]   time: 0.30   mse_train: 0.047263 vq_train: 0.313583 commitment_train: 0.313583\n",
            "Train Epoch: 6 [ 8320/60032 (13%)]   time: 0.32   mse_train: 0.047414 vq_train: 0.313578 commitment_train: 0.313578\n",
            "Train Epoch: 6 [ 8960/60032 (14%)]   time: 0.32   mse_train: 0.047519 vq_train: 0.314438 commitment_train: 0.314438\n",
            "Train Epoch: 6 [ 9600/60032 (15%)]   time: 0.32   mse_train: 0.048051 vq_train: 0.314099 commitment_train: 0.314099\n",
            "Train Epoch: 6 [10240/60032 (17%)]   time: 0.32   mse_train: 0.046622 vq_train: 0.313495 commitment_train: 0.313495\n",
            "Train Epoch: 6 [10880/60032 (18%)]   time: 0.29   mse_train: 0.047564 vq_train: 0.312986 commitment_train: 0.312986\n",
            "Train Epoch: 6 [11520/60032 (19%)]   time: 0.29   mse_train: 0.048038 vq_train: 0.313698 commitment_train: 0.313698\n",
            "Train Epoch: 6 [12160/60032 (20%)]   time: 0.31   mse_train: 0.048562 vq_train: 0.313965 commitment_train: 0.313965\n",
            "Train Epoch: 6 [12800/60032 (21%)]   time: 0.25   mse_train: 0.043972 vq_train: 0.310983 commitment_train: 0.310983\n",
            "Train Epoch: 6 [13440/60032 (22%)]   time: 0.31   mse_train: 0.046263 vq_train: 0.311978 commitment_train: 0.311978\n",
            "Train Epoch: 6 [14080/60032 (23%)]   time: 0.35   mse_train: 0.047345 vq_train: 0.310590 commitment_train: 0.310590\n",
            "Train Epoch: 6 [14720/60032 (24%)]   time: 0.32   mse_train: 0.046150 vq_train: 0.311470 commitment_train: 0.311470\n",
            "Train Epoch: 6 [15360/60032 (25%)]   time: 0.26   mse_train: 0.047034 vq_train: 0.310544 commitment_train: 0.310544\n",
            "Train Epoch: 6 [16000/60032 (26%)]   time: 0.32   mse_train: 0.047756 vq_train: 0.314842 commitment_train: 0.314842\n",
            "Train Epoch: 6 [16640/60032 (27%)]   time: 0.30   mse_train: 0.049144 vq_train: 0.312110 commitment_train: 0.312110\n",
            "Train Epoch: 6 [17280/60032 (28%)]   time: 0.32   mse_train: 0.047416 vq_train: 0.310878 commitment_train: 0.310878\n",
            "Train Epoch: 6 [17920/60032 (29%)]   time: 0.31   mse_train: 0.049148 vq_train: 0.308555 commitment_train: 0.308555\n",
            "Train Epoch: 6 [18560/60032 (30%)]   time: 0.29   mse_train: 0.046126 vq_train: 0.309980 commitment_train: 0.309980\n",
            "Train Epoch: 6 [19200/60032 (31%)]   time: 0.34   mse_train: 0.047213 vq_train: 0.311355 commitment_train: 0.311355\n",
            "Train Epoch: 6 [19840/60032 (33%)]   time: 0.28   mse_train: 0.045441 vq_train: 0.307922 commitment_train: 0.307922\n",
            "Train Epoch: 6 [20480/60032 (34%)]   time: 0.25   mse_train: 0.047294 vq_train: 0.310224 commitment_train: 0.310224\n",
            "Train Epoch: 6 [21120/60032 (35%)]   time: 0.20   mse_train: 0.046120 vq_train: 0.309421 commitment_train: 0.309421\n",
            "Train Epoch: 6 [21760/60032 (36%)]   time: 0.21   mse_train: 0.046799 vq_train: 0.307422 commitment_train: 0.307422\n",
            "Train Epoch: 6 [22400/60032 (37%)]   time: 0.20   mse_train: 0.046765 vq_train: 0.307744 commitment_train: 0.307744\n",
            "Train Epoch: 6 [23040/60032 (38%)]   time: 0.21   mse_train: 0.045837 vq_train: 0.305644 commitment_train: 0.305644\n",
            "Train Epoch: 6 [23680/60032 (39%)]   time: 0.21   mse_train: 0.045687 vq_train: 0.302999 commitment_train: 0.302999\n",
            "Train Epoch: 6 [24320/60032 (40%)]   time: 0.21   mse_train: 0.046474 vq_train: 0.303954 commitment_train: 0.303954\n",
            "Train Epoch: 6 [24960/60032 (41%)]   time: 0.22   mse_train: 0.047945 vq_train: 0.309242 commitment_train: 0.309242\n",
            "Train Epoch: 6 [25600/60032 (42%)]   time: 0.22   mse_train: 0.045262 vq_train: 0.304583 commitment_train: 0.304583\n",
            "Train Epoch: 6 [26240/60032 (43%)]   time: 0.22   mse_train: 0.045609 vq_train: 0.304148 commitment_train: 0.304148\n",
            "Train Epoch: 6 [26880/60032 (44%)]   time: 0.21   mse_train: 0.046822 vq_train: 0.303335 commitment_train: 0.303335\n",
            "Train Epoch: 6 [27520/60032 (45%)]   time: 0.22   mse_train: 0.046480 vq_train: 0.307199 commitment_train: 0.307199\n",
            "Train Epoch: 6 [28160/60032 (46%)]   time: 0.22   mse_train: 0.048341 vq_train: 0.303562 commitment_train: 0.303562\n",
            "Train Epoch: 6 [28800/60032 (47%)]   time: 0.21   mse_train: 0.047530 vq_train: 0.304210 commitment_train: 0.304210\n",
            "Train Epoch: 6 [29440/60032 (49%)]   time: 0.21   mse_train: 0.046791 vq_train: 0.304734 commitment_train: 0.304734\n",
            "Train Epoch: 6 [30080/60032 (50%)]   time: 0.21   mse_train: 0.047439 vq_train: 0.304634 commitment_train: 0.304634\n",
            "Train Epoch: 6 [30720/60032 (51%)]   time: 0.22   mse_train: 0.046228 vq_train: 0.303332 commitment_train: 0.303332\n",
            "Train Epoch: 6 [31360/60032 (52%)]   time: 0.21   mse_train: 0.047203 vq_train: 0.303737 commitment_train: 0.303737\n",
            "Train Epoch: 6 [32000/60032 (53%)]   time: 0.21   mse_train: 0.048123 vq_train: 0.305014 commitment_train: 0.305014\n",
            "Train Epoch: 6 [32640/60032 (54%)]   time: 0.23   mse_train: 0.044689 vq_train: 0.300510 commitment_train: 0.300510\n",
            "Train Epoch: 6 [33280/60032 (55%)]   time: 0.30   mse_train: 0.046633 vq_train: 0.303334 commitment_train: 0.303334\n",
            "Train Epoch: 6 [33920/60032 (56%)]   time: 0.25   mse_train: 0.046811 vq_train: 0.302858 commitment_train: 0.302858\n",
            "Train Epoch: 6 [34560/60032 (57%)]   time: 0.19   mse_train: 0.046748 vq_train: 0.302900 commitment_train: 0.302900\n",
            "Train Epoch: 6 [35200/60032 (58%)]   time: 0.23   mse_train: 0.047466 vq_train: 0.304861 commitment_train: 0.304861\n",
            "Train Epoch: 6 [35840/60032 (59%)]   time: 0.19   mse_train: 0.047046 vq_train: 0.300788 commitment_train: 0.300788\n",
            "Train Epoch: 6 [36480/60032 (60%)]   time: 0.21   mse_train: 0.045195 vq_train: 0.301470 commitment_train: 0.301470\n",
            "Train Epoch: 6 [37120/60032 (61%)]   time: 0.21   mse_train: 0.045066 vq_train: 0.297903 commitment_train: 0.297903\n",
            "Train Epoch: 6 [37760/60032 (62%)]   time: 0.20   mse_train: 0.044421 vq_train: 0.298677 commitment_train: 0.298677\n",
            "Train Epoch: 6 [38400/60032 (63%)]   time: 0.19   mse_train: 0.045259 vq_train: 0.299489 commitment_train: 0.299489\n",
            "Train Epoch: 6 [39040/60032 (65%)]   time: 0.20   mse_train: 0.046255 vq_train: 0.300097 commitment_train: 0.300097\n",
            "Train Epoch: 6 [39680/60032 (66%)]   time: 0.23   mse_train: 0.048089 vq_train: 0.297508 commitment_train: 0.297508\n",
            "Train Epoch: 6 [40320/60032 (67%)]   time: 0.20   mse_train: 0.044376 vq_train: 0.297844 commitment_train: 0.297844\n",
            "Train Epoch: 6 [40960/60032 (68%)]   time: 0.19   mse_train: 0.046965 vq_train: 0.297068 commitment_train: 0.297068\n",
            "Train Epoch: 6 [41600/60032 (69%)]   time: 0.20   mse_train: 0.045665 vq_train: 0.298071 commitment_train: 0.298071\n",
            "Train Epoch: 6 [42240/60032 (70%)]   time: 0.20   mse_train: 0.044858 vq_train: 0.296152 commitment_train: 0.296152\n",
            "Train Epoch: 6 [42880/60032 (71%)]   time: 0.21   mse_train: 0.046366 vq_train: 0.300606 commitment_train: 0.300606\n",
            "Train Epoch: 6 [43520/60032 (72%)]   time: 0.20   mse_train: 0.045662 vq_train: 0.295680 commitment_train: 0.295680\n",
            "Train Epoch: 6 [44160/60032 (73%)]   time: 0.20   mse_train: 0.047828 vq_train: 0.297628 commitment_train: 0.297628\n",
            "Train Epoch: 6 [44800/60032 (74%)]   time: 0.19   mse_train: 0.048205 vq_train: 0.300415 commitment_train: 0.300415\n",
            "Train Epoch: 6 [45440/60032 (75%)]   time: 0.19   mse_train: 0.046749 vq_train: 0.298305 commitment_train: 0.298305\n",
            "Train Epoch: 6 [46080/60032 (76%)]   time: 0.21   mse_train: 0.046386 vq_train: 0.297603 commitment_train: 0.297603\n",
            "Train Epoch: 6 [46720/60032 (77%)]   time: 0.19   mse_train: 0.045965 vq_train: 0.299466 commitment_train: 0.299466\n",
            "Train Epoch: 6 [47360/60032 (78%)]   time: 0.21   mse_train: 0.046685 vq_train: 0.299540 commitment_train: 0.299540\n",
            "Train Epoch: 6 [48000/60032 (79%)]   time: 0.19   mse_train: 0.046681 vq_train: 0.301442 commitment_train: 0.301442\n",
            "Train Epoch: 6 [48640/60032 (81%)]   time: 0.19   mse_train: 0.046714 vq_train: 0.299158 commitment_train: 0.299158\n",
            "Train Epoch: 6 [49280/60032 (82%)]   time: 0.22   mse_train: 0.047112 vq_train: 0.302026 commitment_train: 0.302026\n",
            "Train Epoch: 6 [49920/60032 (83%)]   time: 0.19   mse_train: 0.044971 vq_train: 0.295624 commitment_train: 0.295624\n",
            "Train Epoch: 6 [50560/60032 (84%)]   time: 0.27   mse_train: 0.047163 vq_train: 0.301341 commitment_train: 0.301341\n",
            "Train Epoch: 6 [51200/60032 (85%)]   time: 0.30   mse_train: 0.047964 vq_train: 0.300945 commitment_train: 0.300945\n",
            "Train Epoch: 6 [51840/60032 (86%)]   time: 0.32   mse_train: 0.047212 vq_train: 0.301371 commitment_train: 0.301371\n",
            "Train Epoch: 6 [52480/60032 (87%)]   time: 0.27   mse_train: 0.045359 vq_train: 0.298671 commitment_train: 0.298671\n",
            "Train Epoch: 6 [53120/60032 (88%)]   time: 0.34   mse_train: 0.045326 vq_train: 0.299063 commitment_train: 0.299063\n",
            "Train Epoch: 6 [53760/60032 (89%)]   time: 0.30   mse_train: 0.043824 vq_train: 0.299211 commitment_train: 0.299211\n",
            "Train Epoch: 6 [54400/60032 (90%)]   time: 0.31   mse_train: 0.045071 vq_train: 0.295378 commitment_train: 0.295378\n",
            "Train Epoch: 6 [55040/60032 (91%)]   time: 0.30   mse_train: 0.048842 vq_train: 0.299735 commitment_train: 0.299735\n",
            "Train Epoch: 6 [55680/60032 (92%)]   time: 0.30   mse_train: 0.045655 vq_train: 0.299399 commitment_train: 0.299399\n",
            "Train Epoch: 6 [56320/60032 (93%)]   time: 0.33   mse_train: 0.048212 vq_train: 0.300949 commitment_train: 0.300949\n",
            "Train Epoch: 6 [56960/60032 (94%)]   time: 0.31   mse_train: 0.048750 vq_train: 0.303136 commitment_train: 0.303136\n",
            "Train Epoch: 6 [57600/60032 (95%)]   time: 0.31   mse_train: 0.046388 vq_train: 0.300685 commitment_train: 0.300685\n",
            "Train Epoch: 6 [58240/60032 (97%)]   time: 0.29   mse_train: 0.045645 vq_train: 0.298894 commitment_train: 0.298894\n",
            "Train Epoch: 6 [58880/60032 (98%)]   time: 0.31   mse_train: 0.047025 vq_train: 0.297718 commitment_train: 0.297718\n",
            "Train Epoch: 6 [59520/60032 (99%)]   time: 0.12   mse_train: 0.045433 vq_train: 0.298616 commitment_train: 0.298616\n",
            "====> Epoch: 6 mse_train: 0.046810\tvq_train: 0.305351\tcommitment_train: 0.305351\n",
            "====> Test set losses: mse_test: 0.045663 vq_test: 0.308278 commitment_test: 0.308278\n",
            "Train Epoch: 7 [    0/60032 ( 0%)]   time: 0.65   mse_train: 0.004757 vq_train: 0.029248 commitment_train: 0.029248\n",
            "Train Epoch: 7 [  640/60032 ( 1%)]   time: 0.41   mse_train: 0.046255 vq_train: 0.297103 commitment_train: 0.297103\n",
            "Train Epoch: 7 [ 1280/60032 ( 2%)]   time: 0.31   mse_train: 0.045999 vq_train: 0.300918 commitment_train: 0.300918\n",
            "Train Epoch: 7 [ 1920/60032 ( 3%)]   time: 0.30   mse_train: 0.046111 vq_train: 0.294976 commitment_train: 0.294976\n",
            "Train Epoch: 7 [ 2560/60032 ( 4%)]   time: 0.30   mse_train: 0.047519 vq_train: 0.300731 commitment_train: 0.300731\n",
            "Train Epoch: 7 [ 3200/60032 ( 5%)]   time: 0.30   mse_train: 0.044612 vq_train: 0.295500 commitment_train: 0.295500\n",
            "Train Epoch: 7 [ 3840/60032 ( 6%)]   time: 0.32   mse_train: 0.044597 vq_train: 0.296378 commitment_train: 0.296378\n",
            "Train Epoch: 7 [ 4480/60032 ( 7%)]   time: 0.31   mse_train: 0.045560 vq_train: 0.299243 commitment_train: 0.299243\n",
            "Train Epoch: 7 [ 5120/60032 ( 8%)]   time: 0.32   mse_train: 0.045343 vq_train: 0.299310 commitment_train: 0.299310\n",
            "Train Epoch: 7 [ 5760/60032 ( 9%)]   time: 0.29   mse_train: 0.045491 vq_train: 0.295553 commitment_train: 0.295553\n",
            "Train Epoch: 7 [ 6400/60032 (10%)]   time: 0.32   mse_train: 0.045018 vq_train: 0.297996 commitment_train: 0.297996\n",
            "Train Epoch: 7 [ 7040/60032 (11%)]   time: 0.20   mse_train: 0.046995 vq_train: 0.293236 commitment_train: 0.293236\n",
            "Train Epoch: 7 [ 7680/60032 (12%)]   time: 0.21   mse_train: 0.045918 vq_train: 0.299420 commitment_train: 0.299420\n",
            "Train Epoch: 7 [ 8320/60032 (13%)]   time: 0.18   mse_train: 0.045596 vq_train: 0.297934 commitment_train: 0.297934\n",
            "Train Epoch: 7 [ 8960/60032 (14%)]   time: 0.22   mse_train: 0.045779 vq_train: 0.299818 commitment_train: 0.299818\n",
            "Train Epoch: 7 [ 9600/60032 (15%)]   time: 0.22   mse_train: 0.044996 vq_train: 0.292700 commitment_train: 0.292700\n",
            "Train Epoch: 7 [10240/60032 (17%)]   time: 0.21   mse_train: 0.046441 vq_train: 0.300553 commitment_train: 0.300553\n",
            "Train Epoch: 7 [10880/60032 (18%)]   time: 0.21   mse_train: 0.045188 vq_train: 0.292394 commitment_train: 0.292394\n",
            "Train Epoch: 7 [11520/60032 (19%)]   time: 0.21   mse_train: 0.045590 vq_train: 0.298376 commitment_train: 0.298376\n",
            "Train Epoch: 7 [12160/60032 (20%)]   time: 0.19   mse_train: 0.047041 vq_train: 0.296756 commitment_train: 0.296756\n",
            "Train Epoch: 7 [12800/60032 (21%)]   time: 0.22   mse_train: 0.043702 vq_train: 0.294959 commitment_train: 0.294959\n",
            "Train Epoch: 7 [13440/60032 (22%)]   time: 0.22   mse_train: 0.047511 vq_train: 0.296513 commitment_train: 0.296513\n",
            "Train Epoch: 7 [14080/60032 (23%)]   time: 0.22   mse_train: 0.045647 vq_train: 0.299120 commitment_train: 0.299120\n",
            "Train Epoch: 7 [14720/60032 (24%)]   time: 0.19   mse_train: 0.046657 vq_train: 0.298687 commitment_train: 0.298687\n",
            "Train Epoch: 7 [15360/60032 (25%)]   time: 0.22   mse_train: 0.044773 vq_train: 0.294538 commitment_train: 0.294538\n",
            "Train Epoch: 7 [16000/60032 (26%)]   time: 0.20   mse_train: 0.045839 vq_train: 0.299965 commitment_train: 0.299965\n",
            "Train Epoch: 7 [16640/60032 (27%)]   time: 0.22   mse_train: 0.045909 vq_train: 0.293752 commitment_train: 0.293752\n",
            "Train Epoch: 7 [17280/60032 (28%)]   time: 0.20   mse_train: 0.048125 vq_train: 0.302620 commitment_train: 0.302620\n",
            "Train Epoch: 7 [17920/60032 (29%)]   time: 0.22   mse_train: 0.047388 vq_train: 0.300567 commitment_train: 0.300567\n",
            "Train Epoch: 7 [18560/60032 (30%)]   time: 0.31   mse_train: 0.047107 vq_train: 0.301470 commitment_train: 0.301470\n",
            "Train Epoch: 7 [19200/60032 (31%)]   time: 0.29   mse_train: 0.044776 vq_train: 0.297523 commitment_train: 0.297523\n",
            "Train Epoch: 7 [19840/60032 (33%)]   time: 0.30   mse_train: 0.046391 vq_train: 0.299472 commitment_train: 0.299472\n",
            "Train Epoch: 7 [20480/60032 (34%)]   time: 0.30   mse_train: 0.048518 vq_train: 0.299181 commitment_train: 0.299181\n",
            "Train Epoch: 7 [21120/60032 (35%)]   time: 0.30   mse_train: 0.046539 vq_train: 0.303902 commitment_train: 0.303902\n",
            "Train Epoch: 7 [21760/60032 (36%)]   time: 0.30   mse_train: 0.045137 vq_train: 0.298014 commitment_train: 0.298014\n",
            "Train Epoch: 7 [22400/60032 (37%)]   time: 0.34   mse_train: 0.047102 vq_train: 0.298516 commitment_train: 0.298516\n",
            "Train Epoch: 7 [23040/60032 (38%)]   time: 0.30   mse_train: 0.043676 vq_train: 0.296385 commitment_train: 0.296385\n",
            "Train Epoch: 7 [23680/60032 (39%)]   time: 0.28   mse_train: 0.045709 vq_train: 0.298973 commitment_train: 0.298973\n",
            "Train Epoch: 7 [24320/60032 (40%)]   time: 0.30   mse_train: 0.045104 vq_train: 0.295583 commitment_train: 0.295583\n",
            "Train Epoch: 7 [24960/60032 (41%)]   time: 0.28   mse_train: 0.044594 vq_train: 0.296023 commitment_train: 0.296023\n",
            "Train Epoch: 7 [25600/60032 (42%)]   time: 0.33   mse_train: 0.044918 vq_train: 0.297765 commitment_train: 0.297765\n",
            "Train Epoch: 7 [26240/60032 (43%)]   time: 0.29   mse_train: 0.045342 vq_train: 0.297634 commitment_train: 0.297634\n",
            "Train Epoch: 7 [26880/60032 (44%)]   time: 0.27   mse_train: 0.044781 vq_train: 0.294632 commitment_train: 0.294632\n",
            "Train Epoch: 7 [27520/60032 (45%)]   time: 0.31   mse_train: 0.045934 vq_train: 0.299594 commitment_train: 0.299594\n",
            "Train Epoch: 7 [28160/60032 (46%)]   time: 0.29   mse_train: 0.046814 vq_train: 0.300265 commitment_train: 0.300265\n",
            "Train Epoch: 7 [28800/60032 (47%)]   time: 0.29   mse_train: 0.044695 vq_train: 0.295013 commitment_train: 0.295013\n",
            "Train Epoch: 7 [29440/60032 (49%)]   time: 0.32   mse_train: 0.044869 vq_train: 0.293863 commitment_train: 0.293863\n",
            "Train Epoch: 7 [30080/60032 (50%)]   time: 0.32   mse_train: 0.044594 vq_train: 0.298907 commitment_train: 0.298907\n",
            "Train Epoch: 7 [30720/60032 (51%)]   time: 0.32   mse_train: 0.046572 vq_train: 0.294098 commitment_train: 0.294098\n",
            "Train Epoch: 7 [31360/60032 (52%)]   time: 0.29   mse_train: 0.048664 vq_train: 0.302732 commitment_train: 0.302732\n",
            "Train Epoch: 7 [32000/60032 (53%)]   time: 0.28   mse_train: 0.045099 vq_train: 0.298820 commitment_train: 0.298820\n",
            "Train Epoch: 7 [32640/60032 (54%)]   time: 0.20   mse_train: 0.045572 vq_train: 0.300156 commitment_train: 0.300156\n",
            "Train Epoch: 7 [33280/60032 (55%)]   time: 0.19   mse_train: 0.045256 vq_train: 0.299474 commitment_train: 0.299474\n",
            "Train Epoch: 7 [33920/60032 (56%)]   time: 0.18   mse_train: 0.046135 vq_train: 0.299840 commitment_train: 0.299840\n",
            "Train Epoch: 7 [34560/60032 (57%)]   time: 0.23   mse_train: 0.045777 vq_train: 0.298240 commitment_train: 0.298240\n",
            "Train Epoch: 7 [35200/60032 (58%)]   time: 0.21   mse_train: 0.044933 vq_train: 0.298124 commitment_train: 0.298124\n",
            "Train Epoch: 7 [35840/60032 (59%)]   time: 0.20   mse_train: 0.045512 vq_train: 0.298068 commitment_train: 0.298068\n",
            "Train Epoch: 7 [36480/60032 (60%)]   time: 0.27   mse_train: 0.043852 vq_train: 0.296303 commitment_train: 0.296303\n",
            "Train Epoch: 7 [37120/60032 (61%)]   time: 0.25   mse_train: 0.044179 vq_train: 0.296430 commitment_train: 0.296430\n",
            "Train Epoch: 7 [37760/60032 (62%)]   time: 0.24   mse_train: 0.045759 vq_train: 0.299722 commitment_train: 0.299722\n",
            "Train Epoch: 7 [38400/60032 (63%)]   time: 0.21   mse_train: 0.044765 vq_train: 0.295891 commitment_train: 0.295891\n",
            "Train Epoch: 7 [39040/60032 (65%)]   time: 0.21   mse_train: 0.044647 vq_train: 0.298486 commitment_train: 0.298486\n",
            "Train Epoch: 7 [39680/60032 (66%)]   time: 0.19   mse_train: 0.047029 vq_train: 0.297982 commitment_train: 0.297982\n",
            "Train Epoch: 7 [40320/60032 (67%)]   time: 0.23   mse_train: 0.045888 vq_train: 0.299283 commitment_train: 0.299283\n",
            "Train Epoch: 7 [40960/60032 (68%)]   time: 0.23   mse_train: 0.045371 vq_train: 0.295554 commitment_train: 0.295554\n",
            "Train Epoch: 7 [41600/60032 (69%)]   time: 0.20   mse_train: 0.045229 vq_train: 0.297161 commitment_train: 0.297161\n",
            "Train Epoch: 7 [42240/60032 (70%)]   time: 0.20   mse_train: 0.044987 vq_train: 0.294743 commitment_train: 0.294743\n",
            "Train Epoch: 7 [42880/60032 (71%)]   time: 0.22   mse_train: 0.044023 vq_train: 0.297150 commitment_train: 0.297150\n",
            "Train Epoch: 7 [43520/60032 (72%)]   time: 0.20   mse_train: 0.044958 vq_train: 0.296107 commitment_train: 0.296107\n",
            "Train Epoch: 7 [44160/60032 (73%)]   time: 0.20   mse_train: 0.044350 vq_train: 0.295314 commitment_train: 0.295314\n",
            "Train Epoch: 7 [44800/60032 (74%)]   time: 0.22   mse_train: 0.046860 vq_train: 0.302307 commitment_train: 0.302307\n",
            "Train Epoch: 7 [45440/60032 (75%)]   time: 0.20   mse_train: 0.044442 vq_train: 0.295554 commitment_train: 0.295554\n",
            "Train Epoch: 7 [46080/60032 (76%)]   time: 0.20   mse_train: 0.046319 vq_train: 0.300104 commitment_train: 0.300104\n",
            "Train Epoch: 7 [46720/60032 (77%)]   time: 0.24   mse_train: 0.045063 vq_train: 0.297766 commitment_train: 0.297766\n",
            "Train Epoch: 7 [47360/60032 (78%)]   time: 0.18   mse_train: 0.045325 vq_train: 0.299708 commitment_train: 0.299708\n",
            "Train Epoch: 7 [48000/60032 (79%)]   time: 0.21   mse_train: 0.045035 vq_train: 0.298373 commitment_train: 0.298373\n",
            "Train Epoch: 7 [48640/60032 (81%)]   time: 0.20   mse_train: 0.047246 vq_train: 0.301373 commitment_train: 0.301373\n",
            "Train Epoch: 7 [49280/60032 (82%)]   time: 0.20   mse_train: 0.045836 vq_train: 0.303300 commitment_train: 0.303300\n",
            "Train Epoch: 7 [49920/60032 (83%)]   time: 0.22   mse_train: 0.044391 vq_train: 0.295172 commitment_train: 0.295172\n",
            "Train Epoch: 7 [50560/60032 (84%)]   time: 0.19   mse_train: 0.044942 vq_train: 0.298047 commitment_train: 0.298047\n",
            "Train Epoch: 7 [51200/60032 (85%)]   time: 0.22   mse_train: 0.043580 vq_train: 0.297250 commitment_train: 0.297250\n",
            "Train Epoch: 7 [51840/60032 (86%)]   time: 0.20   mse_train: 0.044088 vq_train: 0.299333 commitment_train: 0.299333\n",
            "Train Epoch: 7 [52480/60032 (87%)]   time: 0.20   mse_train: 0.044807 vq_train: 0.298684 commitment_train: 0.298684\n",
            "Train Epoch: 7 [53120/60032 (88%)]   time: 0.21   mse_train: 0.044593 vq_train: 0.297483 commitment_train: 0.297483\n",
            "Train Epoch: 7 [53760/60032 (89%)]   time: 0.20   mse_train: 0.044053 vq_train: 0.299918 commitment_train: 0.299918\n",
            "Train Epoch: 7 [54400/60032 (90%)]   time: 0.21   mse_train: 0.046026 vq_train: 0.297926 commitment_train: 0.297926\n",
            "Train Epoch: 7 [55040/60032 (91%)]   time: 0.20   mse_train: 0.045956 vq_train: 0.301740 commitment_train: 0.301740\n",
            "Train Epoch: 7 [55680/60032 (92%)]   time: 0.19   mse_train: 0.046211 vq_train: 0.302481 commitment_train: 0.302481\n",
            "Train Epoch: 7 [56320/60032 (93%)]   time: 0.21   mse_train: 0.044340 vq_train: 0.299224 commitment_train: 0.299224\n",
            "Train Epoch: 7 [56960/60032 (94%)]   time: 0.20   mse_train: 0.044022 vq_train: 0.300313 commitment_train: 0.300313\n",
            "Train Epoch: 7 [57600/60032 (95%)]   time: 0.20   mse_train: 0.045898 vq_train: 0.298834 commitment_train: 0.298834\n",
            "Train Epoch: 7 [58240/60032 (97%)]   time: 0.20   mse_train: 0.045362 vq_train: 0.303032 commitment_train: 0.303032\n",
            "Train Epoch: 7 [58880/60032 (98%)]   time: 0.19   mse_train: 0.045399 vq_train: 0.300485 commitment_train: 0.300485\n",
            "Train Epoch: 7 [59520/60032 (99%)]   time: 0.10   mse_train: 0.043926 vq_train: 0.297658 commitment_train: 0.297658\n",
            "====> Epoch: 7 mse_train: 0.045545\tvq_train: 0.298316\tcommitment_train: 0.298316\n",
            "====> Test set losses: mse_test: 0.045323 vq_test: 0.310243 commitment_test: 0.310243\n",
            "Train Epoch: 8 [    0/60032 ( 0%)]   time: 0.64   mse_train: 0.004884 vq_train: 0.030829 commitment_train: 0.030829\n",
            "Train Epoch: 8 [  640/60032 ( 1%)]   time: 0.43   mse_train: 0.043993 vq_train: 0.299300 commitment_train: 0.299300\n",
            "Train Epoch: 8 [ 1280/60032 ( 2%)]   time: 0.31   mse_train: 0.045064 vq_train: 0.299847 commitment_train: 0.299847\n",
            "Train Epoch: 8 [ 1920/60032 ( 3%)]   time: 0.27   mse_train: 0.046249 vq_train: 0.302309 commitment_train: 0.302309\n",
            "Train Epoch: 8 [ 2560/60032 ( 4%)]   time: 0.30   mse_train: 0.045394 vq_train: 0.302781 commitment_train: 0.302781\n",
            "Train Epoch: 8 [ 3200/60032 ( 5%)]   time: 0.20   mse_train: 0.044444 vq_train: 0.301853 commitment_train: 0.301853\n",
            "Train Epoch: 8 [ 3840/60032 ( 6%)]   time: 0.22   mse_train: 0.045854 vq_train: 0.302473 commitment_train: 0.302473\n",
            "Train Epoch: 8 [ 4480/60032 ( 7%)]   time: 0.20   mse_train: 0.045543 vq_train: 0.304437 commitment_train: 0.304437\n",
            "Train Epoch: 8 [ 5120/60032 ( 8%)]   time: 0.20   mse_train: 0.045754 vq_train: 0.304812 commitment_train: 0.304812\n",
            "Train Epoch: 8 [ 5760/60032 ( 9%)]   time: 0.19   mse_train: 0.046181 vq_train: 0.304943 commitment_train: 0.304943\n",
            "Train Epoch: 8 [ 6400/60032 (10%)]   time: 0.21   mse_train: 0.043986 vq_train: 0.300512 commitment_train: 0.300512\n",
            "Train Epoch: 8 [ 7040/60032 (11%)]   time: 0.22   mse_train: 0.044689 vq_train: 0.304516 commitment_train: 0.304516\n",
            "Train Epoch: 8 [ 7680/60032 (12%)]   time: 0.20   mse_train: 0.045532 vq_train: 0.300481 commitment_train: 0.300481\n",
            "Train Epoch: 8 [ 8320/60032 (13%)]   time: 0.19   mse_train: 0.045445 vq_train: 0.303918 commitment_train: 0.303918\n",
            "Train Epoch: 8 [ 8960/60032 (14%)]   time: 0.20   mse_train: 0.044242 vq_train: 0.303138 commitment_train: 0.303138\n",
            "Train Epoch: 8 [ 9600/60032 (15%)]   time: 0.21   mse_train: 0.045358 vq_train: 0.305633 commitment_train: 0.305633\n",
            "Train Epoch: 8 [10240/60032 (17%)]   time: 0.20   mse_train: 0.046023 vq_train: 0.306026 commitment_train: 0.306026\n",
            "Train Epoch: 8 [10880/60032 (18%)]   time: 0.22   mse_train: 0.047351 vq_train: 0.309221 commitment_train: 0.309221\n",
            "Train Epoch: 8 [11520/60032 (19%)]   time: 0.19   mse_train: 0.044825 vq_train: 0.306895 commitment_train: 0.306895\n",
            "Train Epoch: 8 [12160/60032 (20%)]   time: 0.21   mse_train: 0.043930 vq_train: 0.304071 commitment_train: 0.304071\n",
            "Train Epoch: 8 [12800/60032 (21%)]   time: 0.19   mse_train: 0.044055 vq_train: 0.304534 commitment_train: 0.304534\n",
            "Train Epoch: 8 [13440/60032 (22%)]   time: 0.22   mse_train: 0.044047 vq_train: 0.305099 commitment_train: 0.305099\n",
            "Train Epoch: 8 [14080/60032 (23%)]   time: 0.19   mse_train: 0.045066 vq_train: 0.303487 commitment_train: 0.303487\n",
            "Train Epoch: 8 [14720/60032 (24%)]   time: 0.21   mse_train: 0.044190 vq_train: 0.303809 commitment_train: 0.303809\n",
            "Train Epoch: 8 [15360/60032 (25%)]   time: 0.21   mse_train: 0.046650 vq_train: 0.312005 commitment_train: 0.312005\n",
            "Train Epoch: 8 [16000/60032 (26%)]   time: 0.18   mse_train: 0.045050 vq_train: 0.306792 commitment_train: 0.306792\n",
            "Train Epoch: 8 [16640/60032 (27%)]   time: 0.23   mse_train: 0.045613 vq_train: 0.309415 commitment_train: 0.309415\n",
            "Train Epoch: 8 [17280/60032 (28%)]   time: 0.21   mse_train: 0.046217 vq_train: 0.310359 commitment_train: 0.310359\n",
            "Train Epoch: 8 [17920/60032 (29%)]   time: 0.21   mse_train: 0.044638 vq_train: 0.310974 commitment_train: 0.310974\n",
            "Train Epoch: 8 [18560/60032 (30%)]   time: 0.20   mse_train: 0.046662 vq_train: 0.313324 commitment_train: 0.313324\n",
            "Train Epoch: 8 [19200/60032 (31%)]   time: 0.21   mse_train: 0.048478 vq_train: 0.313650 commitment_train: 0.313650\n",
            "Train Epoch: 8 [19840/60032 (33%)]   time: 0.20   mse_train: 0.043413 vq_train: 0.310575 commitment_train: 0.310575\n",
            "Train Epoch: 8 [20480/60032 (34%)]   time: 0.23   mse_train: 0.044777 vq_train: 0.308755 commitment_train: 0.308755\n",
            "Train Epoch: 8 [21120/60032 (35%)]   time: 0.20   mse_train: 0.045918 vq_train: 0.310909 commitment_train: 0.310909\n",
            "Train Epoch: 8 [21760/60032 (36%)]   time: 0.20   mse_train: 0.043953 vq_train: 0.309955 commitment_train: 0.309955\n",
            "Train Epoch: 8 [22400/60032 (37%)]   time: 0.18   mse_train: 0.044869 vq_train: 0.311327 commitment_train: 0.311327\n",
            "Train Epoch: 8 [23040/60032 (38%)]   time: 0.22   mse_train: 0.044931 vq_train: 0.311275 commitment_train: 0.311275\n",
            "Train Epoch: 8 [23680/60032 (39%)]   time: 0.20   mse_train: 0.045293 vq_train: 0.311786 commitment_train: 0.311786\n",
            "Train Epoch: 8 [24320/60032 (40%)]   time: 0.20   mse_train: 0.044042 vq_train: 0.312654 commitment_train: 0.312654\n",
            "Train Epoch: 8 [24960/60032 (41%)]   time: 0.21   mse_train: 0.045076 vq_train: 0.307842 commitment_train: 0.307842\n",
            "Train Epoch: 8 [25600/60032 (42%)]   time: 0.20   mse_train: 0.045824 vq_train: 0.312068 commitment_train: 0.312068\n",
            "Train Epoch: 8 [26240/60032 (43%)]   time: 0.21   mse_train: 0.044227 vq_train: 0.312096 commitment_train: 0.312096\n",
            "Train Epoch: 8 [26880/60032 (44%)]   time: 0.20   mse_train: 0.046043 vq_train: 0.310839 commitment_train: 0.310839\n",
            "Train Epoch: 8 [27520/60032 (45%)]   time: 0.21   mse_train: 0.046237 vq_train: 0.313739 commitment_train: 0.313739\n",
            "Train Epoch: 8 [28160/60032 (46%)]   time: 0.21   mse_train: 0.046384 vq_train: 0.313566 commitment_train: 0.313566\n",
            "Train Epoch: 8 [28800/60032 (47%)]   time: 0.24   mse_train: 0.045885 vq_train: 0.311711 commitment_train: 0.311711\n",
            "Train Epoch: 8 [29440/60032 (49%)]   time: 0.32   mse_train: 0.042659 vq_train: 0.307841 commitment_train: 0.307841\n",
            "Train Epoch: 8 [30080/60032 (50%)]   time: 0.33   mse_train: 0.045734 vq_train: 0.313855 commitment_train: 0.313855\n",
            "Train Epoch: 8 [30720/60032 (51%)]   time: 0.24   mse_train: 0.045270 vq_train: 0.314071 commitment_train: 0.314071\n",
            "Train Epoch: 8 [31360/60032 (52%)]   time: 0.22   mse_train: 0.044080 vq_train: 0.310939 commitment_train: 0.310939\n",
            "Train Epoch: 8 [32000/60032 (53%)]   time: 0.23   mse_train: 0.043863 vq_train: 0.311918 commitment_train: 0.311918\n",
            "Train Epoch: 8 [32640/60032 (54%)]   time: 0.28   mse_train: 0.044965 vq_train: 0.314467 commitment_train: 0.314467\n",
            "Train Epoch: 8 [33280/60032 (55%)]   time: 0.30   mse_train: 0.044879 vq_train: 0.311872 commitment_train: 0.311872\n",
            "Train Epoch: 8 [33920/60032 (56%)]   time: 0.31   mse_train: 0.044165 vq_train: 0.310457 commitment_train: 0.310457\n",
            "Train Epoch: 8 [34560/60032 (57%)]   time: 0.29   mse_train: 0.042524 vq_train: 0.308588 commitment_train: 0.308588\n",
            "Train Epoch: 8 [35200/60032 (58%)]   time: 0.29   mse_train: 0.042583 vq_train: 0.308259 commitment_train: 0.308259\n",
            "Train Epoch: 8 [35840/60032 (59%)]   time: 0.27   mse_train: 0.042730 vq_train: 0.307246 commitment_train: 0.307246\n",
            "Train Epoch: 8 [36480/60032 (60%)]   time: 0.33   mse_train: 0.043985 vq_train: 0.307083 commitment_train: 0.307083\n",
            "Train Epoch: 8 [37120/60032 (61%)]   time: 0.31   mse_train: 0.043816 vq_train: 0.311167 commitment_train: 0.311167\n",
            "Train Epoch: 8 [37760/60032 (62%)]   time: 0.26   mse_train: 0.044614 vq_train: 0.307348 commitment_train: 0.307348\n",
            "Train Epoch: 8 [38400/60032 (63%)]   time: 0.33   mse_train: 0.045894 vq_train: 0.315944 commitment_train: 0.315944\n",
            "Train Epoch: 8 [39040/60032 (65%)]   time: 0.30   mse_train: 0.045120 vq_train: 0.309561 commitment_train: 0.309561\n",
            "Train Epoch: 8 [39680/60032 (66%)]   time: 0.26   mse_train: 0.043856 vq_train: 0.309490 commitment_train: 0.309490\n",
            "Train Epoch: 8 [40320/60032 (67%)]   time: 0.32   mse_train: 0.043710 vq_train: 0.308688 commitment_train: 0.308688\n",
            "Train Epoch: 8 [40960/60032 (68%)]   time: 0.34   mse_train: 0.042487 vq_train: 0.307052 commitment_train: 0.307052\n",
            "Train Epoch: 8 [41600/60032 (69%)]   time: 0.29   mse_train: 0.044838 vq_train: 0.311189 commitment_train: 0.311189\n",
            "Train Epoch: 8 [42240/60032 (70%)]   time: 0.27   mse_train: 0.044087 vq_train: 0.305637 commitment_train: 0.305637\n",
            "Train Epoch: 8 [42880/60032 (71%)]   time: 0.32   mse_train: 0.042817 vq_train: 0.305738 commitment_train: 0.305738\n",
            "Train Epoch: 8 [43520/60032 (72%)]   time: 0.31   mse_train: 0.045143 vq_train: 0.310035 commitment_train: 0.310035\n",
            "Train Epoch: 8 [44160/60032 (73%)]   time: 0.27   mse_train: 0.045040 vq_train: 0.308875 commitment_train: 0.308875\n",
            "Train Epoch: 8 [44800/60032 (74%)]   time: 0.32   mse_train: 0.045801 vq_train: 0.310349 commitment_train: 0.310349\n",
            "Train Epoch: 8 [45440/60032 (75%)]   time: 0.24   mse_train: 0.044050 vq_train: 0.311813 commitment_train: 0.311813\n",
            "Train Epoch: 8 [46080/60032 (76%)]   time: 0.20   mse_train: 0.042608 vq_train: 0.304711 commitment_train: 0.304711\n",
            "Train Epoch: 8 [46720/60032 (77%)]   time: 0.26   mse_train: 0.046335 vq_train: 0.309402 commitment_train: 0.309402\n",
            "Train Epoch: 8 [47360/60032 (78%)]   time: 0.26   mse_train: 0.042995 vq_train: 0.309417 commitment_train: 0.309417\n",
            "Train Epoch: 8 [48000/60032 (79%)]   time: 0.21   mse_train: 0.044392 vq_train: 0.311387 commitment_train: 0.311387\n",
            "Train Epoch: 8 [48640/60032 (81%)]   time: 0.21   mse_train: 0.043875 vq_train: 0.306359 commitment_train: 0.306359\n",
            "Train Epoch: 8 [49280/60032 (82%)]   time: 0.19   mse_train: 0.044345 vq_train: 0.306674 commitment_train: 0.306674\n",
            "Train Epoch: 8 [49920/60032 (83%)]   time: 0.22   mse_train: 0.045491 vq_train: 0.308548 commitment_train: 0.308548\n",
            "Train Epoch: 8 [50560/60032 (84%)]   time: 0.20   mse_train: 0.044206 vq_train: 0.307059 commitment_train: 0.307059\n",
            "Train Epoch: 8 [51200/60032 (85%)]   time: 0.20   mse_train: 0.045699 vq_train: 0.311578 commitment_train: 0.311578\n",
            "Train Epoch: 8 [51840/60032 (86%)]   time: 0.20   mse_train: 0.043867 vq_train: 0.305387 commitment_train: 0.305387\n",
            "Train Epoch: 8 [52480/60032 (87%)]   time: 0.20   mse_train: 0.043466 vq_train: 0.304210 commitment_train: 0.304210\n",
            "Train Epoch: 8 [53120/60032 (88%)]   time: 0.20   mse_train: 0.044741 vq_train: 0.311615 commitment_train: 0.311615\n",
            "Train Epoch: 8 [53760/60032 (89%)]   time: 0.20   mse_train: 0.044975 vq_train: 0.308809 commitment_train: 0.308809\n",
            "Train Epoch: 8 [54400/60032 (90%)]   time: 0.23   mse_train: 0.043470 vq_train: 0.308094 commitment_train: 0.308094\n",
            "Train Epoch: 8 [55040/60032 (91%)]   time: 0.21   mse_train: 0.043054 vq_train: 0.308470 commitment_train: 0.308470\n",
            "Train Epoch: 8 [55680/60032 (92%)]   time: 0.21   mse_train: 0.045710 vq_train: 0.311655 commitment_train: 0.311655\n",
            "Train Epoch: 8 [56320/60032 (93%)]   time: 0.20   mse_train: 0.047017 vq_train: 0.312193 commitment_train: 0.312193\n",
            "Train Epoch: 8 [56960/60032 (94%)]   time: 0.24   mse_train: 0.044850 vq_train: 0.315282 commitment_train: 0.315282\n",
            "Train Epoch: 8 [57600/60032 (95%)]   time: 0.20   mse_train: 0.045757 vq_train: 0.314393 commitment_train: 0.314393\n",
            "Train Epoch: 8 [58240/60032 (97%)]   time: 0.19   mse_train: 0.045035 vq_train: 0.313960 commitment_train: 0.313960\n",
            "Train Epoch: 8 [58880/60032 (98%)]   time: 0.23   mse_train: 0.044042 vq_train: 0.312270 commitment_train: 0.312270\n",
            "Train Epoch: 8 [59520/60032 (99%)]   time: 0.10   mse_train: 0.043958 vq_train: 0.311733 commitment_train: 0.311733\n",
            "====> Epoch: 8 mse_train: 0.044818\tvq_train: 0.308811\tcommitment_train: 0.308811\n",
            "====> Test set losses: mse_test: 0.044465 vq_test: 0.322657 commitment_test: 0.322657\n",
            "Train Epoch: 9 [    0/60032 ( 0%)]   time: 0.36   mse_train: 0.003867 vq_train: 0.030562 commitment_train: 0.030562\n",
            "Train Epoch: 9 [  640/60032 ( 1%)]   time: 0.32   mse_train: 0.044043 vq_train: 0.311740 commitment_train: 0.311740\n",
            "Train Epoch: 9 [ 1280/60032 ( 2%)]   time: 0.20   mse_train: 0.043013 vq_train: 0.309983 commitment_train: 0.309983\n",
            "Train Epoch: 9 [ 1920/60032 ( 3%)]   time: 0.19   mse_train: 0.045724 vq_train: 0.313151 commitment_train: 0.313151\n",
            "Train Epoch: 9 [ 2560/60032 ( 4%)]   time: 0.21   mse_train: 0.043951 vq_train: 0.308713 commitment_train: 0.308713\n",
            "Train Epoch: 9 [ 3200/60032 ( 5%)]   time: 0.22   mse_train: 0.044828 vq_train: 0.314475 commitment_train: 0.314475\n",
            "Train Epoch: 9 [ 3840/60032 ( 6%)]   time: 0.20   mse_train: 0.045185 vq_train: 0.312941 commitment_train: 0.312941\n",
            "Train Epoch: 9 [ 4480/60032 ( 7%)]   time: 0.21   mse_train: 0.043538 vq_train: 0.311627 commitment_train: 0.311627\n",
            "Train Epoch: 9 [ 5120/60032 ( 8%)]   time: 0.21   mse_train: 0.043725 vq_train: 0.313851 commitment_train: 0.313851\n",
            "Train Epoch: 9 [ 5760/60032 ( 9%)]   time: 0.30   mse_train: 0.042033 vq_train: 0.313521 commitment_train: 0.313521\n",
            "Train Epoch: 9 [ 6400/60032 (10%)]   time: 0.32   mse_train: 0.043733 vq_train: 0.313856 commitment_train: 0.313856\n",
            "Train Epoch: 9 [ 7040/60032 (11%)]   time: 0.27   mse_train: 0.043332 vq_train: 0.312650 commitment_train: 0.312650\n",
            "Train Epoch: 9 [ 7680/60032 (12%)]   time: 0.31   mse_train: 0.045079 vq_train: 0.309871 commitment_train: 0.309871\n",
            "Train Epoch: 9 [ 8320/60032 (13%)]   time: 0.31   mse_train: 0.043083 vq_train: 0.312556 commitment_train: 0.312556\n",
            "Train Epoch: 9 [ 8960/60032 (14%)]   time: 0.31   mse_train: 0.045932 vq_train: 0.314352 commitment_train: 0.314352\n",
            "Train Epoch: 9 [ 9600/60032 (15%)]   time: 0.32   mse_train: 0.044884 vq_train: 0.315087 commitment_train: 0.315087\n",
            "Train Epoch: 9 [10240/60032 (17%)]   time: 0.31   mse_train: 0.046303 vq_train: 0.316179 commitment_train: 0.316179\n",
            "Train Epoch: 9 [10880/60032 (18%)]   time: 0.29   mse_train: 0.043502 vq_train: 0.313502 commitment_train: 0.313502\n",
            "Train Epoch: 9 [11520/60032 (19%)]   time: 0.28   mse_train: 0.045399 vq_train: 0.313275 commitment_train: 0.313275\n",
            "Train Epoch: 9 [12160/60032 (20%)]   time: 0.33   mse_train: 0.044508 vq_train: 0.317786 commitment_train: 0.317786\n",
            "Train Epoch: 9 [12800/60032 (21%)]   time: 0.25   mse_train: 0.044346 vq_train: 0.316508 commitment_train: 0.316508\n",
            "Train Epoch: 9 [13440/60032 (22%)]   time: 0.30   mse_train: 0.043740 vq_train: 0.312987 commitment_train: 0.312987\n",
            "Train Epoch: 9 [14080/60032 (23%)]   time: 0.30   mse_train: 0.043931 vq_train: 0.315128 commitment_train: 0.315128\n",
            "Train Epoch: 9 [14720/60032 (24%)]   time: 0.31   mse_train: 0.043268 vq_train: 0.313511 commitment_train: 0.313511\n",
            "Train Epoch: 9 [15360/60032 (25%)]   time: 0.29   mse_train: 0.045200 vq_train: 0.313376 commitment_train: 0.313376\n",
            "Train Epoch: 9 [16000/60032 (26%)]   time: 0.33   mse_train: 0.042122 vq_train: 0.312432 commitment_train: 0.312432\n",
            "Train Epoch: 9 [16640/60032 (27%)]   time: 0.33   mse_train: 0.044730 vq_train: 0.312593 commitment_train: 0.312593\n",
            "Train Epoch: 9 [17280/60032 (28%)]   time: 0.30   mse_train: 0.044024 vq_train: 0.314735 commitment_train: 0.314735\n",
            "Train Epoch: 9 [17920/60032 (29%)]   time: 0.30   mse_train: 0.045326 vq_train: 0.315115 commitment_train: 0.315115\n",
            "Train Epoch: 9 [18560/60032 (30%)]   time: 0.28   mse_train: 0.044054 vq_train: 0.316719 commitment_train: 0.316719\n",
            "Train Epoch: 9 [19200/60032 (31%)]   time: 0.21   mse_train: 0.043012 vq_train: 0.312066 commitment_train: 0.312066\n",
            "Train Epoch: 9 [19840/60032 (33%)]   time: 0.21   mse_train: 0.043845 vq_train: 0.315948 commitment_train: 0.315948\n",
            "Train Epoch: 9 [20480/60032 (34%)]   time: 0.20   mse_train: 0.044763 vq_train: 0.314331 commitment_train: 0.314331\n",
            "Train Epoch: 9 [21120/60032 (35%)]   time: 0.19   mse_train: 0.044618 vq_train: 0.317955 commitment_train: 0.317955\n",
            "Train Epoch: 9 [21760/60032 (36%)]   time: 0.21   mse_train: 0.042007 vq_train: 0.313439 commitment_train: 0.313439\n",
            "Train Epoch: 9 [22400/60032 (37%)]   time: 0.22   mse_train: 0.045261 vq_train: 0.316130 commitment_train: 0.316130\n",
            "Train Epoch: 9 [23040/60032 (38%)]   time: 0.21   mse_train: 0.045242 vq_train: 0.318014 commitment_train: 0.318014\n",
            "Train Epoch: 9 [23680/60032 (39%)]   time: 0.21   mse_train: 0.044764 vq_train: 0.319895 commitment_train: 0.319895\n",
            "Train Epoch: 9 [24320/60032 (40%)]   time: 0.19   mse_train: 0.042949 vq_train: 0.317553 commitment_train: 0.317553\n",
            "Train Epoch: 9 [24960/60032 (41%)]   time: 0.18   mse_train: 0.042975 vq_train: 0.316087 commitment_train: 0.316087\n",
            "Train Epoch: 9 [25600/60032 (42%)]   time: 0.22   mse_train: 0.043467 vq_train: 0.315096 commitment_train: 0.315096\n",
            "Train Epoch: 9 [26240/60032 (43%)]   time: 0.20   mse_train: 0.045507 vq_train: 0.317815 commitment_train: 0.317815\n",
            "Train Epoch: 9 [26880/60032 (44%)]   time: 0.22   mse_train: 0.043377 vq_train: 0.313357 commitment_train: 0.313357\n",
            "Train Epoch: 9 [27520/60032 (45%)]   time: 0.21   mse_train: 0.044766 vq_train: 0.317363 commitment_train: 0.317363\n",
            "Train Epoch: 9 [28160/60032 (46%)]   time: 0.19   mse_train: 0.043794 vq_train: 0.315239 commitment_train: 0.315239\n",
            "Train Epoch: 9 [28800/60032 (47%)]   time: 0.22   mse_train: 0.044121 vq_train: 0.317816 commitment_train: 0.317816\n",
            "Train Epoch: 9 [29440/60032 (49%)]   time: 0.19   mse_train: 0.045188 vq_train: 0.316986 commitment_train: 0.316986\n",
            "Train Epoch: 9 [30080/60032 (50%)]   time: 0.19   mse_train: 0.045054 vq_train: 0.318260 commitment_train: 0.318260\n",
            "Train Epoch: 9 [30720/60032 (51%)]   time: 0.20   mse_train: 0.047113 vq_train: 0.325085 commitment_train: 0.325085\n",
            "Train Epoch: 9 [31360/60032 (52%)]   time: 0.20   mse_train: 0.042553 vq_train: 0.316576 commitment_train: 0.316576\n",
            "Train Epoch: 9 [32000/60032 (53%)]   time: 0.24   mse_train: 0.043638 vq_train: 0.315632 commitment_train: 0.315632\n",
            "Train Epoch: 9 [32640/60032 (54%)]   time: 0.20   mse_train: 0.042065 vq_train: 0.315128 commitment_train: 0.315128\n",
            "Train Epoch: 9 [33280/60032 (55%)]   time: 0.19   mse_train: 0.043728 vq_train: 0.320035 commitment_train: 0.320035\n",
            "Train Epoch: 9 [33920/60032 (56%)]   time: 0.21   mse_train: 0.041712 vq_train: 0.314912 commitment_train: 0.314912\n",
            "Train Epoch: 9 [34560/60032 (57%)]   time: 0.20   mse_train: 0.043847 vq_train: 0.315298 commitment_train: 0.315298\n",
            "Train Epoch: 9 [35200/60032 (58%)]   time: 0.22   mse_train: 0.043365 vq_train: 0.315762 commitment_train: 0.315762\n",
            "Train Epoch: 9 [35840/60032 (59%)]   time: 0.17   mse_train: 0.042159 vq_train: 0.313139 commitment_train: 0.313139\n",
            "Train Epoch: 9 [36480/60032 (60%)]   time: 0.22   mse_train: 0.044176 vq_train: 0.316593 commitment_train: 0.316593\n",
            "Train Epoch: 9 [37120/60032 (61%)]   time: 0.21   mse_train: 0.044190 vq_train: 0.315890 commitment_train: 0.315890\n",
            "Train Epoch: 9 [37760/60032 (62%)]   time: 0.22   mse_train: 0.044497 vq_train: 0.316075 commitment_train: 0.316075\n",
            "Train Epoch: 9 [38400/60032 (63%)]   time: 0.20   mse_train: 0.044694 vq_train: 0.318910 commitment_train: 0.318910\n",
            "Train Epoch: 9 [39040/60032 (65%)]   time: 0.20   mse_train: 0.043960 vq_train: 0.315716 commitment_train: 0.315716\n",
            "Train Epoch: 9 [39680/60032 (66%)]   time: 0.21   mse_train: 0.045835 vq_train: 0.325803 commitment_train: 0.325803\n",
            "Train Epoch: 9 [40320/60032 (67%)]   time: 0.21   mse_train: 0.045003 vq_train: 0.316443 commitment_train: 0.316443\n",
            "Train Epoch: 9 [40960/60032 (68%)]   time: 0.22   mse_train: 0.045462 vq_train: 0.322409 commitment_train: 0.322409\n",
            "Train Epoch: 9 [41600/60032 (69%)]   time: 0.21   mse_train: 0.043547 vq_train: 0.318413 commitment_train: 0.318413\n",
            "Train Epoch: 9 [42240/60032 (70%)]   time: 0.20   mse_train: 0.043038 vq_train: 0.322258 commitment_train: 0.322258\n",
            "Train Epoch: 9 [42880/60032 (71%)]   time: 0.20   mse_train: 0.043996 vq_train: 0.318581 commitment_train: 0.318581\n",
            "Train Epoch: 9 [43520/60032 (72%)]   time: 0.20   mse_train: 0.045375 vq_train: 0.320422 commitment_train: 0.320422\n",
            "Train Epoch: 9 [44160/60032 (73%)]   time: 0.22   mse_train: 0.044495 vq_train: 0.322773 commitment_train: 0.322773\n",
            "Train Epoch: 9 [44800/60032 (74%)]   time: 0.20   mse_train: 0.042841 vq_train: 0.321079 commitment_train: 0.321079\n",
            "Train Epoch: 9 [45440/60032 (75%)]   time: 0.20   mse_train: 0.043556 vq_train: 0.316622 commitment_train: 0.316622\n",
            "Train Epoch: 9 [46080/60032 (76%)]   time: 0.20   mse_train: 0.044622 vq_train: 0.319258 commitment_train: 0.319258\n",
            "Train Epoch: 9 [46720/60032 (77%)]   time: 0.21   mse_train: 0.045953 vq_train: 0.324264 commitment_train: 0.324264\n",
            "Train Epoch: 9 [47360/60032 (78%)]   time: 0.23   mse_train: 0.043236 vq_train: 0.322159 commitment_train: 0.322159\n",
            "Train Epoch: 9 [48000/60032 (79%)]   time: 0.20   mse_train: 0.046032 vq_train: 0.319500 commitment_train: 0.319500\n",
            "Train Epoch: 9 [48640/60032 (81%)]   time: 0.21   mse_train: 0.043674 vq_train: 0.327301 commitment_train: 0.327301\n",
            "Train Epoch: 9 [49280/60032 (82%)]   time: 0.29   mse_train: 0.045435 vq_train: 0.321091 commitment_train: 0.321091\n",
            "Train Epoch: 9 [49920/60032 (83%)]   time: 0.30   mse_train: 0.044619 vq_train: 0.325111 commitment_train: 0.325111\n",
            "Train Epoch: 9 [50560/60032 (84%)]   time: 0.27   mse_train: 0.043237 vq_train: 0.320426 commitment_train: 0.320426\n",
            "Train Epoch: 9 [51200/60032 (85%)]   time: 0.31   mse_train: 0.044169 vq_train: 0.323819 commitment_train: 0.323819\n",
            "Train Epoch: 9 [51840/60032 (86%)]   time: 0.32   mse_train: 0.045442 vq_train: 0.324014 commitment_train: 0.324014\n",
            "Train Epoch: 9 [52480/60032 (87%)]   time: 0.29   mse_train: 0.044368 vq_train: 0.322683 commitment_train: 0.322683\n",
            "Train Epoch: 9 [53120/60032 (88%)]   time: 0.32   mse_train: 0.044071 vq_train: 0.325983 commitment_train: 0.325983\n",
            "Train Epoch: 9 [53760/60032 (89%)]   time: 0.30   mse_train: 0.044655 vq_train: 0.324517 commitment_train: 0.324517\n",
            "Train Epoch: 9 [54400/60032 (90%)]   time: 0.31   mse_train: 0.043575 vq_train: 0.323504 commitment_train: 0.323504\n",
            "Train Epoch: 9 [55040/60032 (91%)]   time: 0.29   mse_train: 0.045178 vq_train: 0.325926 commitment_train: 0.325926\n",
            "Train Epoch: 9 [55680/60032 (92%)]   time: 0.30   mse_train: 0.043764 vq_train: 0.322416 commitment_train: 0.322416\n",
            "Train Epoch: 9 [56320/60032 (93%)]   time: 0.30   mse_train: 0.044378 vq_train: 0.320682 commitment_train: 0.320682\n",
            "Train Epoch: 9 [56960/60032 (94%)]   time: 0.33   mse_train: 0.044558 vq_train: 0.325468 commitment_train: 0.325468\n",
            "Train Epoch: 9 [57600/60032 (95%)]   time: 0.27   mse_train: 0.043307 vq_train: 0.319346 commitment_train: 0.319346\n",
            "Train Epoch: 9 [58240/60032 (97%)]   time: 0.40   mse_train: 0.043161 vq_train: 0.322553 commitment_train: 0.322553\n",
            "Train Epoch: 9 [58880/60032 (98%)]   time: 0.31   mse_train: 0.044903 vq_train: 0.325236 commitment_train: 0.325236\n",
            "Train Epoch: 9 [59520/60032 (99%)]   time: 0.14   mse_train: 0.043856 vq_train: 0.323516 commitment_train: 0.323516\n",
            "====> Epoch: 9 mse_train: 0.044193\tvq_train: 0.317778\tcommitment_train: 0.317778\n",
            "====> Test set losses: mse_test: 0.043972 vq_test: 0.333195 commitment_test: 0.333195\n",
            "Train Epoch: 10 [    0/60032 ( 0%)]   time: 0.38   mse_train: 0.004707 vq_train: 0.031935 commitment_train: 0.031935\n",
            "Train Epoch: 10 [  640/60032 ( 1%)]   time: 0.31   mse_train: 0.044783 vq_train: 0.323002 commitment_train: 0.323002\n",
            "Train Epoch: 10 [ 1280/60032 ( 2%)]   time: 0.20   mse_train: 0.044149 vq_train: 0.321591 commitment_train: 0.321591\n",
            "Train Epoch: 10 [ 1920/60032 ( 3%)]   time: 0.22   mse_train: 0.043347 vq_train: 0.319272 commitment_train: 0.319272\n",
            "Train Epoch: 10 [ 2560/60032 ( 4%)]   time: 0.21   mse_train: 0.044264 vq_train: 0.321667 commitment_train: 0.321667\n",
            "Train Epoch: 10 [ 3200/60032 ( 5%)]   time: 0.22   mse_train: 0.044094 vq_train: 0.322886 commitment_train: 0.322886\n",
            "Train Epoch: 10 [ 3840/60032 ( 6%)]   time: 0.20   mse_train: 0.045396 vq_train: 0.326899 commitment_train: 0.326899\n",
            "Train Epoch: 10 [ 4480/60032 ( 7%)]   time: 0.21   mse_train: 0.042420 vq_train: 0.318026 commitment_train: 0.318026\n",
            "Train Epoch: 10 [ 5120/60032 ( 8%)]   time: 0.23   mse_train: 0.043572 vq_train: 0.320334 commitment_train: 0.320334\n",
            "Train Epoch: 10 [ 5760/60032 ( 9%)]   time: 0.20   mse_train: 0.045307 vq_train: 0.323123 commitment_train: 0.323123\n",
            "Train Epoch: 10 [ 6400/60032 (10%)]   time: 0.23   mse_train: 0.043684 vq_train: 0.319936 commitment_train: 0.319936\n",
            "Train Epoch: 10 [ 7040/60032 (11%)]   time: 0.21   mse_train: 0.045015 vq_train: 0.322241 commitment_train: 0.322241\n",
            "Train Epoch: 10 [ 7680/60032 (12%)]   time: 0.21   mse_train: 0.043436 vq_train: 0.321233 commitment_train: 0.321233\n",
            "Train Epoch: 10 [ 8320/60032 (13%)]   time: 0.22   mse_train: 0.045023 vq_train: 0.325578 commitment_train: 0.325578\n",
            "Train Epoch: 10 [ 8960/60032 (14%)]   time: 0.19   mse_train: 0.042222 vq_train: 0.319373 commitment_train: 0.319373\n",
            "Train Epoch: 10 [ 9600/60032 (15%)]   time: 0.24   mse_train: 0.044743 vq_train: 0.320329 commitment_train: 0.320329\n",
            "Train Epoch: 10 [10240/60032 (17%)]   time: 0.20   mse_train: 0.041744 vq_train: 0.319918 commitment_train: 0.319918\n",
            "Train Epoch: 10 [10880/60032 (18%)]   time: 0.22   mse_train: 0.044244 vq_train: 0.319407 commitment_train: 0.319407\n",
            "Train Epoch: 10 [11520/60032 (19%)]   time: 0.24   mse_train: 0.044786 vq_train: 0.321216 commitment_train: 0.321216\n",
            "Train Epoch: 10 [12160/60032 (20%)]   time: 0.22   mse_train: 0.043671 vq_train: 0.323052 commitment_train: 0.323052\n",
            "Train Epoch: 10 [12800/60032 (21%)]   time: 0.22   mse_train: 0.044948 vq_train: 0.318341 commitment_train: 0.318341\n",
            "Train Epoch: 10 [13440/60032 (22%)]   time: 0.20   mse_train: 0.046586 vq_train: 0.326500 commitment_train: 0.326500\n",
            "Train Epoch: 10 [14080/60032 (23%)]   time: 0.23   mse_train: 0.044622 vq_train: 0.317959 commitment_train: 0.317959\n",
            "Train Epoch: 10 [14720/60032 (24%)]   time: 0.21   mse_train: 0.043442 vq_train: 0.321130 commitment_train: 0.321130\n",
            "Train Epoch: 10 [15360/60032 (25%)]   time: 0.21   mse_train: 0.043806 vq_train: 0.317478 commitment_train: 0.317478\n",
            "Train Epoch: 10 [16000/60032 (26%)]   time: 0.21   mse_train: 0.042679 vq_train: 0.318260 commitment_train: 0.318260\n",
            "Train Epoch: 10 [16640/60032 (27%)]   time: 0.20   mse_train: 0.043027 vq_train: 0.316355 commitment_train: 0.316355\n",
            "Train Epoch: 10 [17280/60032 (28%)]   time: 0.22   mse_train: 0.043312 vq_train: 0.319505 commitment_train: 0.319505\n",
            "Train Epoch: 10 [17920/60032 (29%)]   time: 0.19   mse_train: 0.044829 vq_train: 0.321410 commitment_train: 0.321410\n",
            "Train Epoch: 10 [18560/60032 (30%)]   time: 0.24   mse_train: 0.043950 vq_train: 0.321420 commitment_train: 0.321420\n",
            "Train Epoch: 10 [19200/60032 (31%)]   time: 0.21   mse_train: 0.042439 vq_train: 0.318106 commitment_train: 0.318106\n",
            "Train Epoch: 10 [19840/60032 (33%)]   time: 0.21   mse_train: 0.043105 vq_train: 0.318864 commitment_train: 0.318864\n",
            "Train Epoch: 10 [20480/60032 (34%)]   time: 0.22   mse_train: 0.043387 vq_train: 0.316647 commitment_train: 0.316647\n",
            "Train Epoch: 10 [21120/60032 (35%)]   time: 0.21   mse_train: 0.042842 vq_train: 0.319877 commitment_train: 0.319877\n",
            "Train Epoch: 10 [21760/60032 (36%)]   time: 0.22   mse_train: 0.044365 vq_train: 0.323685 commitment_train: 0.323685\n",
            "Train Epoch: 10 [22400/60032 (37%)]   time: 0.28   mse_train: 0.044208 vq_train: 0.322721 commitment_train: 0.322721\n",
            "Train Epoch: 10 [23040/60032 (38%)]   time: 0.34   mse_train: 0.044431 vq_train: 0.320858 commitment_train: 0.320858\n",
            "Train Epoch: 10 [23680/60032 (39%)]   time: 0.26   mse_train: 0.042194 vq_train: 0.320525 commitment_train: 0.320525\n",
            "Train Epoch: 10 [24320/60032 (40%)]   time: 0.32   mse_train: 0.042313 vq_train: 0.320641 commitment_train: 0.320641\n",
            "Train Epoch: 10 [24960/60032 (41%)]   time: 0.30   mse_train: 0.043211 vq_train: 0.319953 commitment_train: 0.319953\n",
            "Train Epoch: 10 [25600/60032 (42%)]   time: 0.32   mse_train: 0.043260 vq_train: 0.319042 commitment_train: 0.319042\n",
            "Train Epoch: 10 [26240/60032 (43%)]   time: 0.31   mse_train: 0.044219 vq_train: 0.323165 commitment_train: 0.323165\n",
            "Train Epoch: 10 [26880/60032 (44%)]   time: 0.27   mse_train: 0.043976 vq_train: 0.321494 commitment_train: 0.321494\n",
            "Train Epoch: 10 [27520/60032 (45%)]   time: 0.29   mse_train: 0.043496 vq_train: 0.323791 commitment_train: 0.323791\n",
            "Train Epoch: 10 [28160/60032 (46%)]   time: 0.32   mse_train: 0.046316 vq_train: 0.327109 commitment_train: 0.327109\n",
            "Train Epoch: 10 [28800/60032 (47%)]   time: 0.28   mse_train: 0.043513 vq_train: 0.321695 commitment_train: 0.321695\n",
            "Train Epoch: 10 [29440/60032 (49%)]   time: 0.33   mse_train: 0.042391 vq_train: 0.315531 commitment_train: 0.315531\n",
            "Train Epoch: 10 [30080/60032 (50%)]   time: 0.32   mse_train: 0.044074 vq_train: 0.325220 commitment_train: 0.325220\n",
            "Train Epoch: 10 [30720/60032 (51%)]   time: 0.30   mse_train: 0.043600 vq_train: 0.325557 commitment_train: 0.325557\n",
            "Train Epoch: 10 [31360/60032 (52%)]   time: 0.32   mse_train: 0.044721 vq_train: 0.324611 commitment_train: 0.324611\n",
            "Train Epoch: 10 [32000/60032 (53%)]   time: 0.30   mse_train: 0.044020 vq_train: 0.324811 commitment_train: 0.324811\n",
            "Train Epoch: 10 [32640/60032 (54%)]   time: 0.30   mse_train: 0.043773 vq_train: 0.324250 commitment_train: 0.324250\n",
            "Train Epoch: 10 [33280/60032 (55%)]   time: 0.30   mse_train: 0.044234 vq_train: 0.323595 commitment_train: 0.323595\n",
            "Train Epoch: 10 [33920/60032 (56%)]   time: 0.34   mse_train: 0.042733 vq_train: 0.325124 commitment_train: 0.325124\n",
            "Train Epoch: 10 [34560/60032 (57%)]   time: 0.27   mse_train: 0.044632 vq_train: 0.326508 commitment_train: 0.326508\n",
            "Train Epoch: 10 [35200/60032 (58%)]   time: 0.27   mse_train: 0.043073 vq_train: 0.324767 commitment_train: 0.324767\n",
            "Train Epoch: 10 [35840/60032 (59%)]   time: 0.26   mse_train: 0.043360 vq_train: 0.322675 commitment_train: 0.322675\n",
            "Train Epoch: 10 [36480/60032 (60%)]   time: 0.21   mse_train: 0.044483 vq_train: 0.325719 commitment_train: 0.325719\n",
            "Train Epoch: 10 [37120/60032 (61%)]   time: 0.18   mse_train: 0.044869 vq_train: 0.328641 commitment_train: 0.328641\n",
            "Train Epoch: 10 [37760/60032 (62%)]   time: 0.21   mse_train: 0.042518 vq_train: 0.322437 commitment_train: 0.322437\n",
            "Train Epoch: 10 [38400/60032 (63%)]   time: 0.20   mse_train: 0.043790 vq_train: 0.328045 commitment_train: 0.328045\n",
            "Train Epoch: 10 [39040/60032 (65%)]   time: 0.20   mse_train: 0.042598 vq_train: 0.322651 commitment_train: 0.322651\n",
            "Train Epoch: 10 [39680/60032 (66%)]   time: 0.22   mse_train: 0.043339 vq_train: 0.325557 commitment_train: 0.325557\n",
            "Train Epoch: 10 [40320/60032 (67%)]   time: 0.21   mse_train: 0.045119 vq_train: 0.327869 commitment_train: 0.327869\n",
            "Train Epoch: 10 [40960/60032 (68%)]   time: 0.18   mse_train: 0.044482 vq_train: 0.322965 commitment_train: 0.322965\n",
            "Train Epoch: 10 [41600/60032 (69%)]   time: 0.21   mse_train: 0.044894 vq_train: 0.327838 commitment_train: 0.327838\n",
            "Train Epoch: 10 [42240/60032 (70%)]   time: 0.21   mse_train: 0.043035 vq_train: 0.328380 commitment_train: 0.328380\n",
            "Train Epoch: 10 [42880/60032 (71%)]   time: 0.21   mse_train: 0.044017 vq_train: 0.328447 commitment_train: 0.328447\n",
            "Train Epoch: 10 [43520/60032 (72%)]   time: 0.20   mse_train: 0.044408 vq_train: 0.327964 commitment_train: 0.327964\n",
            "Train Epoch: 10 [44160/60032 (73%)]   time: 0.20   mse_train: 0.043486 vq_train: 0.327261 commitment_train: 0.327261\n",
            "Train Epoch: 10 [44800/60032 (74%)]   time: 0.20   mse_train: 0.042467 vq_train: 0.325351 commitment_train: 0.325351\n",
            "Train Epoch: 10 [45440/60032 (75%)]   time: 0.20   mse_train: 0.044169 vq_train: 0.327623 commitment_train: 0.327623\n",
            "Train Epoch: 10 [46080/60032 (76%)]   time: 0.21   mse_train: 0.043185 vq_train: 0.328357 commitment_train: 0.328357\n",
            "Train Epoch: 10 [46720/60032 (77%)]   time: 0.20   mse_train: 0.044012 vq_train: 0.329195 commitment_train: 0.329195\n",
            "Train Epoch: 10 [47360/60032 (78%)]   time: 0.19   mse_train: 0.043885 vq_train: 0.328586 commitment_train: 0.328586\n",
            "Train Epoch: 10 [48000/60032 (79%)]   time: 0.20   mse_train: 0.042229 vq_train: 0.327007 commitment_train: 0.327007\n",
            "Train Epoch: 10 [48640/60032 (81%)]   time: 0.22   mse_train: 0.042096 vq_train: 0.327364 commitment_train: 0.327364\n",
            "Train Epoch: 10 [49280/60032 (82%)]   time: 0.21   mse_train: 0.043547 vq_train: 0.324852 commitment_train: 0.324852\n",
            "Train Epoch: 10 [49920/60032 (83%)]   time: 0.22   mse_train: 0.042196 vq_train: 0.325724 commitment_train: 0.325724\n",
            "Train Epoch: 10 [50560/60032 (84%)]   time: 0.20   mse_train: 0.043829 vq_train: 0.327035 commitment_train: 0.327035\n",
            "Train Epoch: 10 [51200/60032 (85%)]   time: 0.19   mse_train: 0.042267 vq_train: 0.325266 commitment_train: 0.325266\n",
            "Train Epoch: 10 [51840/60032 (86%)]   time: 0.22   mse_train: 0.042639 vq_train: 0.325752 commitment_train: 0.325752\n",
            "Train Epoch: 10 [52480/60032 (87%)]   time: 0.22   mse_train: 0.044573 vq_train: 0.332297 commitment_train: 0.332297\n",
            "Train Epoch: 10 [53120/60032 (88%)]   time: 0.21   mse_train: 0.045192 vq_train: 0.329436 commitment_train: 0.329436\n",
            "Train Epoch: 10 [53760/60032 (89%)]   time: 0.21   mse_train: 0.044549 vq_train: 0.330086 commitment_train: 0.330086\n",
            "Train Epoch: 10 [54400/60032 (90%)]   time: 0.20   mse_train: 0.043439 vq_train: 0.329914 commitment_train: 0.329914\n",
            "Train Epoch: 10 [55040/60032 (91%)]   time: 0.21   mse_train: 0.043784 vq_train: 0.327647 commitment_train: 0.327647\n",
            "Train Epoch: 10 [55680/60032 (92%)]   time: 0.23   mse_train: 0.043861 vq_train: 0.331000 commitment_train: 0.331000\n",
            "Train Epoch: 10 [56320/60032 (93%)]   time: 0.20   mse_train: 0.043612 vq_train: 0.327043 commitment_train: 0.327043\n",
            "Train Epoch: 10 [56960/60032 (94%)]   time: 0.22   mse_train: 0.044027 vq_train: 0.328932 commitment_train: 0.328932\n",
            "Train Epoch: 10 [57600/60032 (95%)]   time: 0.20   mse_train: 0.044818 vq_train: 0.333045 commitment_train: 0.333045\n",
            "Train Epoch: 10 [58240/60032 (97%)]   time: 0.21   mse_train: 0.042957 vq_train: 0.328978 commitment_train: 0.328978\n",
            "Train Epoch: 10 [58880/60032 (98%)]   time: 0.20   mse_train: 0.041844 vq_train: 0.327373 commitment_train: 0.327373\n",
            "Train Epoch: 10 [59520/60032 (99%)]   time: 0.12   mse_train: 0.043275 vq_train: 0.331868 commitment_train: 0.331868\n",
            "====> Epoch: 10 mse_train: 0.043773\tvq_train: 0.324169\tcommitment_train: 0.324169\n",
            "====> Test set losses: mse_test: 0.043843 vq_test: 0.330064 commitment_test: 0.330064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **13) Save the model**\n",
        "\n",
        "Save the final model to the path **saved_path**\n",
        "\n",
        "**!!!** Uncomment and change to the desired path **!!!**"
      ],
      "metadata": {
        "id": "I0deuwJpV1RI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# saved_path = '/content/gdrive/MyDrive/ENSTA PARIS/IA323/'\n",
        "\n",
        "# saved_path = os.path.join(saved_path, 'final_model.pth')\n",
        "\n",
        "# torch.save(modelTrained.state_dict(), saved_path)"
      ],
      "metadata": {
        "id": "YOOIJs1PVhJl"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}